time=2025-12-30T16:54:10.202-05:00 level=INFO source=routes.go:1544 msg="server config" env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/alexandercpaul/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
time=2025-12-30T16:54:10.207-05:00 level=INFO source=images.go:522 msg="total blobs: 18"
time=2025-12-30T16:54:10.207-05:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-12-30T16:54:10.208-05:00 level=INFO source=routes.go:1597 msg="Listening on 127.0.0.1:11434 (version 0.13.1)"
time=2025-12-30T16:54:10.209-05:00 level=INFO source=runner.go:67 msg="discovering available GPUs..."
time=2025-12-30T16:54:10.213-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --ollama-engine --port 52198"
time=2025-12-30T16:54:10.333-05:00 level=INFO source=types.go:42 msg="inference compute" id=0 filter_id=0 library=Metal compute=0.0 name=Metal description="Apple M4 Pro" libdirs="" driver=0.0 pci_id="" type=discrete total="17.8 GiB" available="17.8 GiB"
time=2025-12-30T16:54:10.333-05:00 level=INFO source=routes.go:1638 msg="entering low vram mode" "total vram"="17.8 GiB" threshold="20.0 GiB"
[GIN] 2025/12/30 - 16:54:12 | 200 |         869µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:54:12 | 200 |    3.451625ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/30 - 16:54:27 | 200 |      31.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:54:27 | 200 |   34.107333ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 16:58:24 | 200 |       20.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:58:24 | 200 |  605.396584ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 16:58:25 | 200 |      18.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:58:25 | 200 |     26.8505ms |       127.0.0.1 | POST     "/api/show"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T16:58:25.281-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 52977"
time=2025-12-30T16:58:25.285-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="6.5 GiB" free_swap="0 B"
time=2025-12-30T16:58:25.285-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T16:58:25.285-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T16:58:25.286-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T16:58:25.286-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-30T16:58:25.286-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-30T16:58:25.286-05:00 level=INFO source=device.go:272 msg="total memory" size="5.1 GiB"
time=2025-12-30T16:58:25.342-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.020 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T16:58:25.343-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T16:58:25.379-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:52977"
time=2025-12-30T16:58:25.386-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-12-30T16:58:25.386-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T16:58:25.386-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T16:58:28.398-05:00 level=INFO source=server.go:1332 msg="llama runner started in 3.11 seconds"
time=2025-12-30T16:58:28.399-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T16:58:28.399-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T16:58:28.399-05:00 level=INFO source=server.go:1332 msg="llama runner started in 3.11 seconds"
[GIN] 2025/12/30 - 16:58:50 | 200 | 25.567313208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 16:58:54 | 200 |     680.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:58:54 | 200 |   24.280584ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 16:58:54 | 200 |      15.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 16:58:54 | 200 |    26.91525ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T16:58:54.765-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 53094"
time=2025-12-30T16:58:54.768-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="6.4 GiB" free_swap="0 B"
time=2025-12-30T16:58:54.768-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T16:58:54.768-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T16:58:54.768-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T16:58:54.769-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="1.0 GiB"
time=2025-12-30T16:58:54.769-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="560.0 MiB"
time=2025-12-30T16:58:54.769-05:00 level=INFO source=device.go:272 msg="total memory" size="5.8 GiB"
time=2025-12-30T16:58:54.779-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.006 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T16:58:54.780-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T16:58:54.817-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:53094"
time=2025-12-30T16:58:54.823-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T16:58:54.823-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T16:58:54.823-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      Metal compute buffer size =   548.01 MiB
llama_context:        CPU compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T16:58:55.577-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.81 seconds"
time=2025-12-30T16:58:55.577-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T16:58:55.577-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T16:58:55.577-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.81 seconds"
[GIN] 2025/12/30 - 16:59:25 | 200 | 31.302420542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:06:55 | 200 |      36.791µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:06:55 | 200 |   24.160292ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:07:18 | 200 |      17.917µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:07:19 | 200 |    29.19375ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T17:07:19.233-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 54401"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="8.5 GiB" free_swap="0 B"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T17:07:19.236-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-30T17:07:19.236-05:00 level=INFO source=device.go:272 msg="total memory" size="5.1 GiB"
time=2025-12-30T17:07:19.326-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T17:07:19.326-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T17:07:19.372-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:54401"
time=2025-12-30T17:07:19.379-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T17:07:19.379-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:07:19.380-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T17:07:22.141-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.91 seconds"
time=2025-12-30T17:07:22.141-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T17:07:22.141-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:07:22.142-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.91 seconds"
[GIN] 2025/12/30 - 17:07:30 | 200 | 11.899896584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:08:47 | 200 |       52.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:08:47 | 200 |   22.062875ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:08:56 | 200 |      14.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:08:56 | 200 |   26.358417ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:01 | 200 |  4.935007042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:01 | 200 |      12.666µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:01 | 200 |   24.109208ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:04 | 200 |  3.236492792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:04 | 200 |      23.334µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:04 | 200 |   23.854542ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:05 | 200 |   1.02689175s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:05 | 200 |      15.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:05 | 200 |   24.189125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:09 | 200 |  3.584139625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:09 | 200 |      13.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:09 | 200 |   24.105875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:12 | 200 |  3.231277458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:12 | 200 |          15µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:12 | 200 |   24.304125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:14 | 200 |  2.031758416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:14 | 200 |          16µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:14 | 200 |   23.993458ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:15 | 200 |  821.081125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:15 | 200 |      14.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:15 | 200 |     23.9545ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:24 | 200 |  8.836849959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:24 | 200 |      14.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:24 | 200 |      23.991ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:26 | 200 |    1.5356405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:09:26 | 200 |       13.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:09:26 | 200 |   25.057625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:09:27 | 200 |  1.565884833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:08 | 200 |        15.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:08 | 200 |   27.471084ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:11 | 200 |  2.701438417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:11 | 200 |      15.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:11 | 200 |    24.59375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:12 | 200 |  885.248291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:12 | 200 |      14.291µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:12 | 200 |   25.986458ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:13 | 200 |  839.087584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:13 | 200 |      14.458µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:13 | 200 |   24.671833ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:14 | 200 |  1.345466875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:14 | 200 |      14.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:14 | 200 |   23.813667ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:15 | 200 |  825.747209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:53 | 200 |      18.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:53 | 200 |   27.135334ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:55 | 200 |     2.698559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:55 | 200 |          17µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:56 | 200 |   26.103875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:56 | 200 |   743.76025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:56 | 200 |        14.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:56 | 200 |   23.573291ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:57 | 200 |   1.03126625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:57 | 200 |      14.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:57 | 200 |   24.885959ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:10:58 | 200 |  761.679041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:10:58 | 200 |      13.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:10:58 | 200 |   24.274208ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:11:01 | 200 |  2.666945791s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:12:20 | 200 |          44µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:12:20 | 200 |   29.111625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:12:21 | 200 |  1.547362625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:12:21 | 200 |      14.333µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:12:21 | 200 |   23.242042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:12:22 | 200 |  750.308042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:12:22 | 200 |          15µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:12:22 | 200 |   24.090583ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:12:23 | 200 |  1.015421083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:31:01 | 200 |      44.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:31:01 | 200 |   29.969042ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:31:01 | 200 |      18.875µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:31:01 | 200 |   25.605541ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T17:31:01.778-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 58216"
time=2025-12-30T17:31:01.781-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="8.8 GiB" free_swap="0 B"
time=2025-12-30T17:31:01.781-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T17:31:01.781-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T17:31:01.782-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T17:31:01.782-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-30T17:31:01.782-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-30T17:31:01.782-05:00 level=INFO source=device.go:272 msg="total memory" size="5.1 GiB"
time=2025-12-30T17:31:01.843-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T17:31:01.843-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T17:31:01.899-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:58216"
time=2025-12-30T17:31:01.905-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T17:31:01.905-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:31:01.905-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T17:31:04.917-05:00 level=INFO source=server.go:1332 msg="llama runner started in 3.14 seconds"
time=2025-12-30T17:31:04.917-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T17:31:04.917-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:31:04.918-05:00 level=INFO source=server.go:1332 msg="llama runner started in 3.14 seconds"
[GIN] 2025/12/30 - 17:31:31 | 200 | 29.967232125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:31:34 | 200 |       18.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:31:34 | 200 |   31.862458ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:31:34 | 200 |      19.375µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:31:34 | 200 |   28.340333ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T17:31:35.046-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 58352"
time=2025-12-30T17:31:35.050-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="7.3 GiB" free_swap="0 B"
time=2025-12-30T17:31:35.051-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T17:31:35.051-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T17:31:35.051-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T17:31:35.051-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="1.0 GiB"
time=2025-12-30T17:31:35.051-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="560.0 MiB"
time=2025-12-30T17:31:35.051-05:00 level=INFO source=device.go:272 msg="total memory" size="5.8 GiB"
time=2025-12-30T17:31:35.060-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.006 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T17:31:35.061-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T17:31:35.087-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:58352"
time=2025-12-30T17:31:35.095-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T17:31:35.095-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:31:35.095-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      Metal compute buffer size =   548.01 MiB
llama_context:        CPU compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T17:31:35.849-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.80 seconds"
time=2025-12-30T17:31:35.849-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T17:31:35.849-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:31:35.849-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.80 seconds"
[GIN] 2025/12/30 - 17:32:04 | 200 |    29.964779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:33 | 200 |      51.416µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:33 | 200 |   27.547958ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:33:33 | 200 |      14.458µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:33 | 200 |   26.636459ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T17:33:34.099-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 58775"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="7.4 GiB" free_swap="0 B"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T17:33:34.103-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-30T17:33:34.103-05:00 level=INFO source=device.go:272 msg="total memory" size="5.1 GiB"
time=2025-12-30T17:33:34.113-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.006 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T17:33:34.113-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T17:33:34.142-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:58775"
time=2025-12-30T17:33:34.147-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T17:33:34.147-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:33:34.147-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T17:33:34.649-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.55 seconds"
time=2025-12-30T17:33:34.650-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T17:33:34.650-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:33:34.650-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.55 seconds"
[GIN] 2025/12/30 - 17:33:40 | 200 |  6.766397875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:40 | 200 |       13.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:40 | 200 |   25.517541ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:42 | 200 |  1.900240709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:42 | 200 |      15.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:42 | 200 |    24.40725ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:43 | 200 |  753.319084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:43 | 200 |      14.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:43 | 200 |   23.927042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:47 | 200 |  3.918892542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:47 | 200 |      13.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:47 | 200 |    23.76125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:48 | 200 |  747.218208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:48 | 200 |      14.959µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:48 | 200 |   24.104625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:48 | 200 |  594.013583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:48 | 200 |      14.917µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:48 | 200 |   23.900542ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:49 | 200 |   882.44975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:49 | 200 |      13.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:49 | 200 |   27.429917ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:50 | 200 |  749.525708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:50 | 200 |      14.416µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:50 | 200 |   24.622708ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:51 | 200 |  1.251002583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:51 | 200 |      14.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:51 | 200 |   24.670041ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:52 | 200 |  758.935292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:52 | 200 |      13.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:52 | 200 |   24.860459ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:57 | 200 |  4.482928583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:57 | 200 |      15.292µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:57 | 200 |   24.856542ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:33:57 | 200 |  743.896917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:33:57 | 200 |      14.291µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:33:57 | 200 |   24.266875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:00 | 200 |  2.637772708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:00 | 200 |      16.291µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:00 | 200 |   25.672334ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:04 | 200 |  3.849437292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:04 | 200 |      15.334µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:04 | 200 |   24.541125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:06 | 200 |  2.203864208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:08 | 200 |      33.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:08 | 200 |   29.605167ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/30 - 17:34:08 | 200 |      13.791µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:08 | 200 |   25.056166ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T17:34:09.084-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 58911"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="8.1 GiB" free_swap="0 B"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-30T17:34:09.088-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="1.0 GiB"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="560.0 MiB"
time=2025-12-30T17:34:09.088-05:00 level=INFO source=device.go:272 msg="total memory" size="5.8 GiB"
time=2025-12-30T17:34:09.098-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.006 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T17:34:09.099-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T17:34:09.130-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:58911"
time=2025-12-30T17:34:09.132-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T17:34:09.132-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:34:09.132-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      Metal compute buffer size =   548.01 MiB
llama_context:        CPU compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-30T17:34:09.885-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.80 seconds"
time=2025-12-30T17:34:09.885-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T17:34:09.885-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T17:34:09.886-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.80 seconds"
[GIN] 2025/12/30 - 17:34:34 | 200 |  25.99023325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:34 | 200 |      16.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:34 | 200 |   30.423791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:39 | 200 |  4.760548333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:39 | 200 |      15.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:39 | 200 |   26.687958ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:43 | 200 |  4.060399833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:43 | 200 |          14µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:43 | 200 |   24.920875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:46 | 200 |  2.285099458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:46 | 200 |        13.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:46 | 200 |   24.266709ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:47 | 200 |  1.872039625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:47 | 200 |      13.208µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:48 | 200 |   23.812125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:51 | 200 |  3.965635708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:51 | 200 |        13.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:52 | 200 |   25.329583ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:34:55 | 200 |  3.805876833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:34:55 | 200 |      13.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:34:55 | 200 |   23.874167ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:00 | 200 |  4.953338792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:00 | 200 |      14.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:00 | 200 |   24.609583ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:03 | 200 |  2.726793792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:03 | 200 |      15.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:03 | 200 |   25.029417ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:07 | 200 |    4.0241425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:07 | 200 |       12.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:07 | 200 |   24.630083ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:08 | 200 |  988.547708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:08 | 200 |      14.041µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:08 | 200 |   24.669375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:13 | 200 |  4.702237167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:13 | 200 |      13.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:13 | 200 |   23.557958ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:15 | 200 |  1.606344167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:15 | 200 |      11.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:15 | 200 |   23.766125ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:19 | 200 |   3.91540325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:35:19 | 200 |      13.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:35:19 | 200 |   24.149708ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 17:35:22 | 200 |  3.239872416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 17:49:43 | 200 |      60.042µs |       127.0.0.1 | HEAD     "/"
time=2025-12-30T17:49:44.280-05:00 level=INFO source=download.go:177 msg="downloading 60e05f210007 in 16 292 MB part(s)"
[GIN] 2025/12/30 - 17:49:45 | 200 |      15.416µs |       127.0.0.1 | HEAD     "/"
time=2025-12-30T17:49:46.019-05:00 level=INFO source=download.go:177 msg="downloading ea025c107c1c in 16 291 MB part(s)"
[GIN] 2025/12/30 - 17:49:46 | 200 |  3.104321458s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/30 - 17:49:48 | 200 |  3.403187292s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/30 - 17:59:40 | 200 |      16.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 17:59:40 | 200 |    3.640959ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/30 - 18:00:50 | 200 |        16.5µs |       127.0.0.1 | HEAD     "/"
time=2025-12-30T18:00:51.341-05:00 level=INFO source=download.go:177 msg="downloading 60e05f210007 in 16 292 MB part(s)"
time=2025-12-30T18:01:23.542-05:00 level=INFO source=download.go:295 msg="60e05f210007 part 1 attempt 0 failed: unexpected EOF, retrying in 1s"
time=2025-12-30T18:02:14.756-05:00 level=INFO source=download.go:177 msg="downloading 66b9ea09bd5b in 1 68 B part(s)"
time=2025-12-30T18:02:16.055-05:00 level=INFO source=download.go:177 msg="downloading 1e65450c3067 in 1 1.6 KB part(s)"
time=2025-12-30T18:02:17.239-05:00 level=INFO source=download.go:177 msg="downloading 832dd9e00a68 in 1 11 KB part(s)"
time=2025-12-30T18:02:18.552-05:00 level=INFO source=download.go:177 msg="downloading d9bb33f27869 in 1 487 B part(s)"
[GIN] 2025/12/30 - 18:02:21 | 200 |         1m30s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/30 - 18:02:29 | 200 |      15.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:02:29 | 200 |    3.660917ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/30 - 18:02:30 | 200 |      16.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:02:30 | 200 |       70.75µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/12/30 - 18:02:38 | 200 |      14.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:02:38 | 200 |   27.698792ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-30T18:02:38.263-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 62322"
time=2025-12-30T18:02:38.266-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="10.6 GiB" free_swap="0 B"
time=2025-12-30T18:02:38.267-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-30T18:02:38.267-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-30T18:02:38.267-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-30T18:02:38.267-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-30T18:02:38.267-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-30T18:02:38.267-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-30T18:02:38.295-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-30T18:02:38.295-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-30T18:02:38.333-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:62322"
time=2025-12-30T18:02:38.344-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-30T18:02:38.344-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T18:02:38.344-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-30T18:02:38.847-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.58 seconds"
time=2025-12-30T18:02:38.847-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-30T18:02:38.847-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-30T18:02:38.847-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.58 seconds"
[GIN] 2025/12/30 - 18:02:39 | 200 |  1.165390584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/30 - 18:02:41 | 200 |      26.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:02:41 | 200 |      24.333µs |       127.0.0.1 | GET      "/api/ps"
[GIN] 2025/12/30 - 18:03:03 | 200 |       30.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:03:03 | 200 |   35.658417ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 18:03:05 | 200 |          20µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:03:05 | 200 |   35.887166ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/30 - 18:14:33 | 200 |          15µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:14:33 | 200 |    3.234291ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/30 - 18:19:16 | 200 |      20.792µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/30 - 18:19:16 | 200 |   18.387292ms |       127.0.0.1 | POST     "/api/create"
[GIN] 2025/12/31 - 00:54:42 | 200 |    1.354834ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 00:54:42 | 200 |    13.18175ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 00:54:46 | 200 |      23.042µs |       127.0.0.1 | HEAD     "/"
time=2025-12-31T00:54:47.357-05:00 level=INFO source=download.go:177 msg="downloading 170370233dd5 in 16 256 MB part(s)"
time=2025-12-31T00:55:44.155-05:00 level=INFO source=download.go:177 msg="downloading 72d6f08a42f6 in 7 100 MB part(s)"
time=2025-12-31T00:55:54.619-05:00 level=INFO source=download.go:177 msg="downloading 43070e2d4e53 in 1 11 KB part(s)"
time=2025-12-31T00:55:55.876-05:00 level=INFO source=download.go:177 msg="downloading c43332387573 in 1 67 B part(s)"
time=2025-12-31T00:55:57.083-05:00 level=INFO source=download.go:177 msg="downloading ed11eda7790d in 1 30 B part(s)"
time=2025-12-31T00:55:58.256-05:00 level=INFO source=download.go:177 msg="downloading 7c658f9561e5 in 1 564 B part(s)"
[GIN] 2025/12/31 - 00:56:01 | 200 |         1m14s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/31 - 00:56:03 | 200 |      31.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 00:56:04 | 200 |   35.886708ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-12-31T00:56:04.049-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 --port 63975"
time=2025-12-31T00:56:04.051-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="8.4 GiB" free_swap="0 B"
time=2025-12-31T00:56:04.051-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T00:56:04.051-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-31T00:56:04.052-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="3.8 GiB"
time=2025-12-31T00:56:04.052-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-31T00:56:04.052-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-31T00:56:04.052-05:00 level=INFO source=device.go:272 msg="total memory" size="4.5 GiB"
time=2025-12-31T00:56:04.087-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T00:56:04.088-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T00:56:04.125-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:63975"
time=2025-12-31T00:56:04.128-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath:/Users/alexandercpaul/.ollama/models/blobs/sha256-72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539 MainGPU:0 UseMmap:true}"
time=2025-12-31T00:56:04.128-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T00:56:04.128-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    70.31 MiB
load_tensors: Metal_Mapped model buffer size =  3847.55 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
clip_model_loader: model name:   openai/clip-vit-large-patch14-336
clip_model_loader: description:  image encoder for LLaVA
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    377
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
clip_ctx: CLIP using Metal backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1024
load_hparams: n_head:             16
load_hparams: n_ff:               4096
load_hparams: n_layer:            23
load_hparams: ffn_op:             gelu_quick
load_hparams: projection_dim:     768

--- vision hparams ---
load_hparams: image_size:         336
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: proj_scale_factor:  0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         595.49 MiB
load_hparams: metadata size:      0.13 MiB
load_tensors: ffn up/down are swapped
alloc_compute_meta:      Metal compute buffer size =    30.63 MiB
alloc_compute_meta:        CPU compute buffer size =     1.30 MiB
time=2025-12-31T00:56:04.881-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.83 seconds"
time=2025-12-31T00:56:04.881-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T00:56:04.881-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T00:56:04.882-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.83 seconds"
[GIN] 2025/12/31 - 00:56:05 | 200 |  1.785249625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 00:56:33 | 200 |     662.084µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 00:56:33 | 200 |   31.035875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 00:56:35 | 200 |   2.04582375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:08:04 | 200 |      55.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:08:04 | 200 |    6.967834ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 01:08:20 | 200 |      22.625µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:08:20 | 200 |   54.775708ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:08:20.580-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 65286"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="11.3 GiB" free_swap="0 B"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T01:08:20.584-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="1.9 GiB"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="448.0 MiB"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="256.5 MiB"
time=2025-12-31T01:08:20.584-05:00 level=INFO source=device.go:272 msg="total memory" size="2.6 GiB"
time=2025-12-31T01:08:20.600-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:08:20.600-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:08:20.641-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:65286"
time=2025-12-31T01:08:20.651-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:08:20.651-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:08:20.651-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
load_tensors: Metal_Mapped model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      Metal compute buffer size =   262.50 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1014
llama_context: graph splits = 2
time=2025-12-31T01:08:21.908-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.32 seconds"
time=2025-12-31T01:08:21.908-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T01:08:21.908-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:08:21.908-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.32 seconds"
[GIN] 2025/12/31 - 01:08:22 | 200 |  1.772557125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:11:03 | 200 |      25.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:11:03 | 200 |   25.303875ms |       127.0.0.1 | POST     "/api/show"
time=2025-12-31T01:11:03.706-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="15.2 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:11:03.729-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 --port 49228"
time=2025-12-31T01:11:03.731-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="9.0 GiB" free_swap="0 B"
time=2025-12-31T01:11:03.731-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="14.7 GiB" free="15.2 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:11:03.731-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-31T01:11:03.732-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="3.8 GiB"
time=2025-12-31T01:11:03.732-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-31T01:11:03.732-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-31T01:11:03.732-05:00 level=INFO source=device.go:272 msg="total memory" size="4.5 GiB"
time=2025-12-31T01:11:03.758-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:11:03.758-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:11:03.797-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:49228"
time=2025-12-31T01:11:03.799-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath:/Users/alexandercpaul/.ollama/models/blobs/sha256-72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539 MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:11:03.799-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:11:03.799-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    70.31 MiB
load_tensors: Metal_Mapped model buffer size =  3847.55 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
clip_model_loader: model name:   openai/clip-vit-large-patch14-336
clip_model_loader: description:  image encoder for LLaVA
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    377
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
clip_ctx: CLIP using Metal backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1024
load_hparams: n_head:             16
load_hparams: n_ff:               4096
load_hparams: n_layer:            23
load_hparams: ffn_op:             gelu_quick
load_hparams: projection_dim:     768

--- vision hparams ---
load_hparams: image_size:         336
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: proj_scale_factor:  0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         595.49 MiB
load_hparams: metadata size:      0.13 MiB
load_tensors: ffn up/down are swapped
alloc_compute_meta:      Metal compute buffer size =    30.63 MiB
alloc_compute_meta:        CPU compute buffer size =     1.30 MiB
time=2025-12-31T01:11:06.562-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.83 seconds"
time=2025-12-31T01:11:06.562-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=2
time=2025-12-31T01:11:06.562-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:11:06.562-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.83 seconds"
[GIN] 2025/12/31 - 01:11:12 | 200 |  8.649535792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:11:25 | 200 |      28.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:11:25 | 200 |    8.743625ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 01:11:27 | 200 |      25.833µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:11:28 | 200 |    574.4085ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/31 - 01:11:59 | 200 |  3.192711542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:12:05 | 200 |  1.020350292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:12:18 | 404 |     178.209µs |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:12:27 | 200 |  548.399083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:12:44 | 200 |  588.002166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:12:53 | 200 |  1.342971375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:39:16 | 200 |       66.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:39:16 | 200 |   23.446041ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:39:16.588-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 --port 51545"
time=2025-12-31T01:39:16.591-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="9.9 GiB" free_swap="0 B"
time=2025-12-31T01:39:16.591-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:39:16.591-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-31T01:39:16.592-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="3.8 GiB"
time=2025-12-31T01:39:16.592-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-31T01:39:16.592-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-31T01:39:16.592-05:00 level=INFO source=device.go:272 msg="total memory" size="4.5 GiB"
time=2025-12-31T01:39:16.666-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:39:16.667-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:39:16.704-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:51545"
time=2025-12-31T01:39:16.714-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath:/Users/alexandercpaul/.ollama/models/blobs/sha256-72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539 MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:39:16.714-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:16.714-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    70.31 MiB
load_tensors: Metal_Mapped model buffer size =  3847.55 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
clip_model_loader: model name:   openai/clip-vit-large-patch14-336
clip_model_loader: description:  image encoder for LLaVA
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    377
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
clip_ctx: CLIP using Metal backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1024
load_hparams: n_head:             16
load_hparams: n_ff:               4096
load_hparams: n_layer:            23
load_hparams: ffn_op:             gelu_quick
load_hparams: projection_dim:     768

--- vision hparams ---
load_hparams: image_size:         336
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: proj_scale_factor:  0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         595.49 MiB
load_hparams: metadata size:      0.13 MiB
load_tensors: ffn up/down are swapped
[GIN] 2025/12/31 - 01:39:19 | 200 |      39.167µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:39:19 | 200 |   34.636792ms |       127.0.0.1 | POST     "/api/show"
alloc_compute_meta:      Metal compute buffer size =    30.63 MiB
alloc_compute_meta:        CPU compute buffer size =     1.30 MiB
time=2025-12-31T01:39:19.477-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.89 seconds"
time=2025-12-31T01:39:19.477-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T01:39:19.477-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:19.477-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.89 seconds"
[GIN] 2025/12/31 - 01:39:19 | 200 |  2.925981167s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T01:39:19.485-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="13.2 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:39:19.636-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 51572"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="5.5 GiB" free_swap="0 B"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="12.7 GiB" free="13.2 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T01:39:19.640-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="1.9 GiB"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="448.0 MiB"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="256.5 MiB"
time=2025-12-31T01:39:19.640-05:00 level=INFO source=device.go:272 msg="total memory" size="2.6 GiB"
time=2025-12-31T01:39:19.650-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:39:19.650-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:39:19.688-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:51572"
time=2025-12-31T01:39:19.693-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:39:19.694-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:19.694-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
load_tensors: Metal_Mapped model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      Metal compute buffer size =   262.50 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1014
llama_context: graph splits = 2
time=2025-12-31T01:39:20.949-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.31 seconds"
time=2025-12-31T01:39:20.949-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=2
time=2025-12-31T01:39:20.949-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:20.949-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.31 seconds"
[GIN] 2025/12/31 - 01:39:20 | 200 |  1.790688625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:39:25 | 200 |      28.416µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:39:25 | 404 |    1.852791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 01:39:26 | 200 |  439.488625ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/12/31 - 01:39:26 | 200 |   48.602833ms |       127.0.0.1 | POST     "/api/show"
time=2025-12-31T01:39:26.433-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="10.7 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:39:26.547-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 51590"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="3.9 GiB" free_swap="0 B"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="10.2 GiB" free="10.7 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T01:39:26.551-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-31T01:39:26.551-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-31T01:39:26.561-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
[GIN] 2025/12/31 - 01:39:28 | 200 |      13.958µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:39:28 | 200 |   25.805917ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 01:39:28 | 200 |   51.909791ms |       127.0.0.1 | POST     "/api/generate"
ggml_metal_library_init: loaded in 5.247 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:39:26.561-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:39:31.832-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:51590"
time=2025-12-31T01:39:31.835-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:39:31.836-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:31.836-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-31T01:39:35.101-05:00 level=INFO source=server.go:1332 msg="llama runner started in 8.55 seconds"
time=2025-12-31T01:39:35.105-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=3
time=2025-12-31T01:39:35.107-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:39:35.109-05:00 level=INFO source=server.go:1332 msg="llama runner started in 8.56 seconds"
[GIN] 2025/12/31 - 01:39:35 | 200 |  8.727909584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:40:17 | 200 |    5.112708ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:40:17 | 200 |   49.623875ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 01:40:17 | 200 |   26.948625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 01:40:18 | 200 |  127.899417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:46:26 | 200 |    1.563916ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:46:26 | 200 |   24.355667ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:46:26.283-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 --port 52275"
time=2025-12-31T01:46:26.287-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="10.2 GiB" free_swap="0 B"
time=2025-12-31T01:46:26.287-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:46:26.287-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-31T01:46:26.288-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="3.8 GiB"
time=2025-12-31T01:46:26.288-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="512.0 MiB"
time=2025-12-31T01:46:26.288-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="296.0 MiB"
time=2025-12-31T01:46:26.288-05:00 level=INFO source=device.go:272 msg="total memory" size="4.5 GiB"
time=2025-12-31T01:46:26.340-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:46:26.340-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:46:26.382-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:52275"
time=2025-12-31T01:46:26.387-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath:/Users/alexandercpaul/.ollama/models/blobs/sha256-72d6f08a42f656d36b356dbe0920675899a99ce21192fd66266fb7d82ed07539 MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:46:26.387-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:26.388-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-170370233dd5c5415250a2ecd5c71586352850729062ccef1496385647293868 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = liuhaotian
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 2
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...
llama_model_loader: - kv  23:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type q4_0:  225 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 3.83 GiB (4.54 BPW) 
load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
load: printing all EOG tokens:
load:   - 2 ('</s>')
load: special tokens cache size = 3
load: token to piece cache size = 0.1637 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.24 B
print_info: general.name     = liuhaotian
print_info: vocab type       = SPM
print_info: n_vocab          = 32000
print_info: n_merges         = 0
print_info: BOS token        = 1 '<s>'
print_info: EOS token        = 2 '</s>'
print_info: UNK token        = 0 '<unk>'
print_info: PAD token        = 0 '<unk>'
print_info: LF token         = 13 '<0x0A>'
print_info: EOG token        = 2 '</s>'
print_info: max token length = 48
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =    70.31 MiB
load_tensors: Metal_Mapped model buffer size =  3847.55 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.14 MiB
llama_kv_cache:      Metal KV buffer size =   512.00 MiB
llama_kv_cache: size =  512.00 MiB (  4096 cells,  32 layers,  1/1 seqs), K (f16):  256.00 MiB, V (f16):  256.00 MiB
llama_context:      Metal compute buffer size =   284.01 MiB
llama_context:        CPU compute buffer size =    20.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
clip_model_loader: model name:   openai/clip-vit-large-patch14-336
clip_model_loader: description:  image encoder for LLaVA
clip_model_loader: GGUF version: 3
clip_model_loader: alignment:    32
clip_model_loader: n_tensors:    377
clip_model_loader: n_kv:         19

clip_model_loader: has vision encoder
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
clip_ctx: CLIP using Metal backend
load_hparams: projector:          mlp
load_hparams: n_embd:             1024
load_hparams: n_head:             16
load_hparams: n_ff:               4096
load_hparams: n_layer:            23
load_hparams: ffn_op:             gelu_quick
load_hparams: projection_dim:     768

--- vision hparams ---
load_hparams: image_size:         336
load_hparams: patch_size:         14
load_hparams: has_llava_proj:     1
load_hparams: minicpmv_version:   0
load_hparams: proj_scale_factor:  0
load_hparams: n_wa_pattern:       0

load_hparams: model size:         595.49 MiB
load_hparams: metadata size:      0.13 MiB
load_tensors: ffn up/down are swapped
alloc_compute_meta:      Metal compute buffer size =    30.63 MiB
alloc_compute_meta:        CPU compute buffer size =     1.30 MiB
time=2025-12-31T01:46:27.141-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.85 seconds"
time=2025-12-31T01:46:27.141-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T01:46:27.141-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:27.142-05:00 level=INFO source=server.go:1332 msg="llama runner started in 0.85 seconds"
[GIN] 2025/12/31 - 01:46:27 | 200 |  1.197743375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:46:28 | 200 |       28.25µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:46:28 | 200 |     46.8225ms |       127.0.0.1 | POST     "/api/show"
time=2025-12-31T01:46:28.820-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="13.2 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:46:28.969-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 52288"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="7.8 GiB" free_swap="0 B"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="12.7 GiB" free="13.2 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T01:46:28.973-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="1.9 GiB"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="448.0 MiB"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="256.5 MiB"
time=2025-12-31T01:46:28.973-05:00 level=INFO source=device.go:272 msg="total memory" size="2.6 GiB"
time=2025-12-31T01:46:28.984-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.018 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:46:28.984-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:46:29.023-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:52288"
time=2025-12-31T01:46:29.027-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:46:29.027-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:29.027-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
load_tensors: Metal_Mapped model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      Metal compute buffer size =   262.50 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1014
llama_context: graph splits = 2
time=2025-12-31T01:46:30.032-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.06 seconds"
time=2025-12-31T01:46:30.032-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=2
time=2025-12-31T01:46:30.032-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:30.032-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.06 seconds"
[GIN] 2025/12/31 - 01:46:30 | 200 |  1.767383625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:46:31 | 200 |      35.791µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:46:31 | 200 |   47.018916ms |       127.0.0.1 | POST     "/api/show"
time=2025-12-31T01:46:31.255-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="10.7 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T01:46:31.373-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 52299"
time=2025-12-31T01:46:31.376-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="7.8 GiB" free_swap="0 B"
time=2025-12-31T01:46:31.376-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="10.2 GiB" free="10.7 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T01:46:31.376-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T01:46:31.377-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-31T01:46:31.377-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-31T01:46:31.377-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-31T01:46:31.377-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-31T01:46:31.388-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T01:46:31.388-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T01:46:31.423-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:52299"
time=2025-12-31T01:46:31.432-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T01:46:31.432-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:31.432-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-31T01:46:32.940-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.56 seconds"
time=2025-12-31T01:46:32.940-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=3
time=2025-12-31T01:46:32.940-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T01:46:32.940-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.56 seconds"
[GIN] 2025/12/31 - 01:46:33 | 200 |  2.263136084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:46:33 | 200 |      40.542µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 01:46:33 | 200 |   45.683292ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 01:46:34 | 200 |  308.040209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:21 | 200 |  1.069249375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:22 | 200 |  593.461458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:25 | 200 |  794.926417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:27 | 200 |  286.100125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:32 | 200 |  560.807875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:34 | 200 |  298.458334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:37 | 200 |  668.541625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:38 | 200 |  242.457208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:44 | 200 |  583.819583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:45 | 200 |  287.406084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:49 | 200 |  713.581708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:50 | 200 |  293.538084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:56 | 200 |  1.125749542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:48:56 | 200 |  345.055292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:00 | 200 |     537.762ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:01 | 200 |     227.616ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:07 | 200 |  428.825083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:08 | 200 |  295.069292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:12 | 200 |  494.036666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:12 | 200 |  231.257666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:19 | 200 |  476.623833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:19 | 200 |  295.901708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:23 | 200 |    466.5415ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:24 | 200 |  261.608708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:30 | 200 |  564.723834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:30 | 200 |   333.63775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:35 | 200 |  283.208292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:35 | 200 |  921.224292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:42 | 200 |  359.298375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:42 | 200 |  969.157583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:46 | 200 |  214.922209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:47 | 200 |  498.333625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:53 | 200 |  216.171583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:54 | 200 |  886.954792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:57 | 200 |  219.849375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:49:58 | 200 |  555.991833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:02 | 200 |  476.018667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:04 | 200 |   267.33525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:05 | 200 |  169.393709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:06 | 200 |  989.841959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:08 | 200 |  513.787167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:09 | 200 |  221.859542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:10 | 200 |   281.52575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:11 | 200 |  218.116792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:13 | 200 |  467.995667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:16 | 200 |  269.356166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:16 | 200 |  156.870209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:18 | 200 |  447.644292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:20 | 200 |  457.803833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:20 | 200 |  290.062167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:22 | 200 |  789.066208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:22 | 200 |  228.482292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:25 | 200 |  468.703625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:27 | 200 |     240.003ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:27 | 200 |   141.27425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:29 | 200 |  555.507458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:31 | 200 |  329.390459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:31 | 200 |      520.69ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:33 | 200 |   625.90325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:33 | 200 |   192.98425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:37 | 200 |  921.457833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:38 | 200 |  228.879333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:38 | 200 |  170.804791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:41 | 200 |  415.509292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:43 | 200 |  265.860625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:43 | 200 |  567.382125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:45 | 200 |  265.557167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:45 | 200 |  662.220958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:48 | 200 |  456.120542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:49 | 200 |  241.406541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:50 | 200 |  239.504042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:53 | 200 |  1.059533667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:54 | 200 |  267.400041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:55 | 200 |   480.97075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:56 | 200 |  234.339792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:50:56 | 200 |  338.820708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:00 | 200 |  466.578625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:01 | 200 |  265.879042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:01 | 200 |  257.424208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:05 | 200 |  1.014746416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:05 | 200 |  195.466792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:07 | 200 |  1.294179209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:07 | 200 |   248.57675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:08 | 200 |  263.430542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:11 | 200 |  418.469041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:12 | 200 |   244.40925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:12 | 200 |  238.415792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:16 | 200 |  284.216209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:16 | 200 |  400.537917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:18 | 200 |   439.50275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:18 | 200 |  208.082208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:19 | 200 |  262.182125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:23 | 200 |  919.704084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:23 | 200 |     262.144ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:23 | 200 |  251.887042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:28 | 200 |  306.466333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:28 | 200 |  369.099583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:30 | 200 |  269.761458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:30 | 200 |  528.217792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:30 | 200 |  252.489875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:34 | 200 |    368.4885ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:35 | 200 |  250.454875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:35 | 200 |  251.784458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:39 | 200 |    280.5225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:40 | 200 |  1.004457292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:41 | 200 |  224.410875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:41 | 200 |  563.971541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:42 | 200 |   472.04575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:46 | 200 |  355.848625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:46 | 200 |  634.429583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:46 | 200 |  416.079958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:50 | 200 |  225.368042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:52 | 200 |  912.984667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:52 | 200 |  216.533541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:53 | 200 |     882.386ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:54 | 200 |  881.942875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:57 | 200 |  401.536292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:58 | 200 |  502.447333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:51:58 | 200 |  477.491417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:02 | 200 |  215.351042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:03 | 200 |  320.011333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:04 | 200 |  227.686916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:05 | 200 |  441.760084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:05 | 200 |  441.682917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:09 | 200 |  224.532042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:09 | 200 |  149.280792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:09 | 200 |  390.905125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:13 | 200 |  215.671917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:15 | 200 |     324.195ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:15 | 200 |   189.05625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:16 | 200 |  460.331875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:17 | 200 |  459.328458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:20 | 200 |    223.9825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:20 | 200 |  175.033208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:20 | 200 |  431.306042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:24 | 200 |  212.885916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:26 | 200 |  356.163417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:26 | 200 |   245.54625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:28 | 200 |  546.006125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:28 | 200 |  549.875375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:31 | 200 |  228.626166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:31 | 200 |  230.128417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:32 | 200 |  436.033833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:35 | 200 |  221.536167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:37 | 200 |  288.311416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:37 | 200 |  397.396875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:39 | 200 |  547.302667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:40 | 200 |  537.939291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:42 | 200 |  230.271458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:43 | 200 |  238.097958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:43 | 200 |  419.774958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:47 | 200 |   210.69825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:49 | 200 |    308.8175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:49 | 200 |  630.162625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:51 | 200 |  529.555125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:51 | 200 |  525.905292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:54 | 200 |  220.883458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:54 | 200 |  220.362834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:55 | 200 |  554.173125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:52:58 | 200 |  204.108375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:00 | 200 |    213.3225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:00 | 200 |  319.343042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:03 | 200 |  529.802833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:03 | 200 |  527.368916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:05 | 200 |  234.395875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:05 | 200 |  234.291917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:07 | 200 |   931.38275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:09 | 200 |   211.11975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:11 | 200 |  211.449208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:12 | 200 |  323.973708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:15 | 200 |  984.586167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:15 | 200 |  976.685916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:16 | 200 |   224.93625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:16 | 200 |  221.516167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:19 | 200 |  957.162167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:20 | 200 |   247.23225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:22 | 200 |   233.11475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:23 | 200 |  387.487292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:26 | 200 |  488.227041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:27 | 200 |  744.760459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:27 | 200 |  242.428583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:28 | 200 |    248.8475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:30 | 200 |   433.34125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:32 | 200 |  282.162833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:34 | 200 |  248.642667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:35 | 200 |  359.267708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:38 | 200 |  703.551542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:39 | 200 |    997.0495ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:39 | 200 |  225.893583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:39 | 200 |  221.805458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:42 | 200 |  844.547125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:43 | 200 |  234.141709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:45 | 200 |  212.642625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:46 | 200 |  319.954666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:49 | 200 |  450.196167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:50 | 200 |   297.85425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:50 | 200 |  268.414416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:50 | 200 |  288.668583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:54 | 200 |  734.308333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:54 | 200 |  220.435708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:56 | 200 |  210.220625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:53:57 | 200 |  546.585792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:01 | 200 |  455.507209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:01 | 200 |  316.230792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:01 | 200 |  372.974583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:02 | 200 |  355.644417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:05 | 200 |  262.314583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:06 | 200 |  1.225960333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:07 | 200 |   238.40575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:09 | 200 |  308.916875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:12 | 200 |  491.104083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:13 | 200 |   261.03225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:13 | 200 |  301.823291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:13 | 200 |  302.224875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:17 | 200 |  218.303333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:18 | 200 |    619.4015ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:19 | 200 |  223.215959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:20 | 200 |  316.276959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:24 | 200 |  566.319041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:24 | 200 |  324.702208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:25 | 200 |  1.018574458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:25 | 200 |   747.50175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:28 | 200 |  213.374583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:30 | 200 |  778.138584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:30 | 200 |  218.816667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:31 | 200 |  312.654209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:36 | 200 |  373.494875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:36 | 200 |  649.440917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:36 | 200 |  217.111916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:36 | 200 |  731.909833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:39 | 200 |   216.04675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:41 | 200 |  301.961666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:42 | 200 |  1.144119375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:43 | 200 |  262.370166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:47 | 200 |  239.865625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:47 | 200 |  233.348875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:48 | 200 |  1.110467958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:49 | 200 |     1.361637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:50 | 200 |  237.578167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:53 | 200 |  223.315292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:54 | 200 |  614.368416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:54 | 200 |  297.667167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:58 | 200 |   210.21175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:58 | 200 |  184.792459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:54:59 | 200 |  522.525625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:00 | 200 |    308.0565ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:02 | 200 |  224.745875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:04 | 200 |  218.454334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:06 | 200 |  1.220028792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:06 | 200 |  896.193125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:09 | 200 |  206.888209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:10 | 200 |  158.292125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:11 | 200 |  502.166625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:12 | 200 |  795.263667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:13 | 200 |  202.388333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:15 | 200 |  262.398708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:18 | 200 |   709.85975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:18 | 200 |  1.421814709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:21 | 200 |  224.286583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:21 | 200 |  143.574125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:23 | 200 |    1.2047405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:23 | 200 |  345.518458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:24 | 200 |  252.766666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:26 | 200 |  229.134584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:29 | 200 |     416.203ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:30 | 200 |  241.495416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:32 | 200 |  224.179042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:32 | 200 |  143.731375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:35 | 200 |  512.517708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:35 | 200 |  495.225166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:35 | 200 |  254.192542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:38 | 200 |   220.25125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:40 | 200 |  399.722458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:41 | 200 |  260.777834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:43 | 200 |  265.045375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:43 | 200 |  282.367333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:46 | 200 |  464.051708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:46 | 200 |  467.239959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:47 | 200 |  243.527625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:49 | 200 |  220.819791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:52 | 200 |  431.270458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:52 | 200 |  254.798459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:54 | 200 |  228.602416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:54 | 200 |  211.421708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:58 | 200 |  530.553334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:58 | 200 |  531.173792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:55:58 | 200 |  208.736292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:00 | 200 |  219.607916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:03 | 200 |  442.266666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:04 | 200 |  217.287667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:06 | 200 |  253.650334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:06 | 200 |  249.335333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:09 | 200 |  495.099209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:09 | 200 |  321.547375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:09 | 200 |  552.678375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:11 | 200 |  229.758375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:16 | 200 |  1.213540625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:16 | 200 |  1.213080375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:17 | 200 |  235.897959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:17 | 200 |  240.910334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:21 | 200 |  281.761125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:21 | 200 |     524.152ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:21 | 200 |  467.226125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:23 | 200 |  217.244791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:28 | 200 |   1.20781275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:28 | 200 |  1.192355209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:28 | 200 |  245.145542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:28 | 200 |    254.2305ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:32 | 200 |  246.692083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:33 | 200 |     1.090973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:33 | 200 |  1.092814125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:34 | 200 |   219.27025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:39 | 200 |  453.479584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:39 | 200 |  309.556833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:40 | 200 |    553.8125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:40 | 200 |  361.578291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:43 | 200 |  222.424958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:44 | 200 |     540.417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:45 | 200 |  528.303333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:45 | 200 |   238.14625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:51 | 200 |  475.749916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:51 | 200 |  457.483209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:51 | 200 |     499.464ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:52 | 200 |   1.02670575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:54 | 200 |  232.952625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:56 | 200 |  523.201541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:56 | 200 |  520.872166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:56:56 | 200 |  193.772917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:02 | 200 |   405.17025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:02 | 200 |  263.191542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:02 | 200 |  164.200625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:04 | 200 |   892.00675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:06 | 200 |  244.849167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:07 | 200 |  497.935709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:08 | 200 |  280.916334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:08 | 200 |  1.003866125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:14 | 200 |   220.40075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:14 | 200 |   214.22675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:14 | 200 |  594.549333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:15 | 200 |   243.58925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:17 | 200 |     252.898ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:19 | 200 |  225.721208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:20 | 200 |  1.099785167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:20 | 200 |  1.130403583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:25 | 200 |    204.1205ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:25 | 200 |  205.656458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:25 | 200 |  415.765958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:26 | 200 |  252.295625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:28 | 200 |  228.610708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:30 | 200 |  227.955083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:31 | 200 |  566.384416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:32 | 200 |  287.008083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:36 | 200 |  399.722667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:36 | 200 |  397.954166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:37 | 200 |  556.113042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:37 | 200 |  258.746458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:40 | 200 |  224.058416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:41 | 200 |  224.452209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:43 | 200 |   654.77425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:43 | 200 |     736.057ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:47 | 200 |  217.887083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:48 | 200 |  240.917958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:48 | 200 |  409.170541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:49 | 200 |  257.165958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:51 | 200 |     222.531ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:53 | 200 |  241.570792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:54 | 200 |  518.591042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:55 | 200 |   264.52125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:59 | 200 |  236.254542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:57:59 | 200 |  242.957042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:01 | 200 |  1.288774375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:01 | 200 |  1.126596667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:02 | 200 |  229.802083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:04 | 200 |  222.504166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:06 | 200 |  513.363083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:06 | 200 |  419.227042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:10 | 200 |  255.809667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:10 | 200 |  236.863792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:12 | 200 |  689.325375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:13 | 200 |  698.613167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:13 | 200 |  298.731291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:15 | 200 |  226.296375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:18 | 200 |  659.686416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:18 | 200 |   919.44025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:21 | 200 |  270.032708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:21 | 200 |    271.8555ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:24 | 200 |  532.061583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:24 | 200 |  514.675291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:25 | 200 |  301.551291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:26 | 200 |  238.406167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:30 | 200 |  989.114333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:30 | 200 |   740.45925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:33 | 200 |    262.9505ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:33 | 200 |  262.537833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:35 | 200 |  453.509583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:36 | 200 |     448.884ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:36 | 200 |  262.268875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:38 | 200 |  224.356458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:41 | 200 |  474.388333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:41 | 200 |  475.530541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:44 | 200 |  215.267083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:44 | 200 |  228.010834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:47 | 200 |  301.896292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:48 | 200 |  1.176027375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:48 | 200 |  1.176934166s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:49 | 200 |  220.196208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:53 | 200 |  485.392458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:53 | 200 |  483.395333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:55 | 200 |   227.34925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:55 | 200 |  228.999709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:58:59 | 200 |  232.765167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:00 | 200 |  1.111996167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:00 | 200 |  1.119129583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:00 | 200 |  196.142125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:04 | 200 |   599.34925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:05 | 200 |  603.545083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:06 | 200 |  299.848417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:06 | 200 |  308.954083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:10 | 200 |   241.06675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:11 | 200 |  398.589291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:11 | 200 |  441.152834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:11 | 200 |  253.859375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:16 | 200 |  485.928666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:16 | 200 |  473.095833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:18 | 200 |  228.700209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:18 | 200 |   223.21425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:21 | 200 |  206.305833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:23 | 200 |  424.725208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:23 | 200 |  305.860708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:23 | 200 |  442.601208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:27 | 200 |  518.078583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:28 | 200 |  511.169375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:29 | 200 |  253.140542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:29 | 200 |  252.545792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:32 | 200 |  244.446291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:34 | 200 |  313.920833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:34 | 200 |  634.003292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:34 | 200 |  567.902209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:39 | 200 |  993.466209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:40 | 200 |  1.000151625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:40 | 200 |    276.4515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:40 | 200 |  279.664166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:44 | 200 |  228.187125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:45 | 200 |  225.174125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:46 | 200 |  1.135426709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:47 | 200 |  1.138386791s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:51 | 200 |   540.32475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:51 | 200 |  523.634125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:52 | 200 |  236.064917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:52 | 200 |  235.902916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:55 | 200 |  214.241875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:57 | 200 |  217.887459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:58 | 200 |  924.155541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 01:59:59 | 200 |  917.729958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:02 | 200 |  480.836209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:03 | 200 |  494.811917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:03 | 200 |  320.411417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:03 | 200 |   326.34525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:06 | 200 |  229.955125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:08 | 200 |  205.996834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:10 | 200 |  596.615667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:10 | 200 |  605.920667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:14 | 200 |     537.045ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:15 | 200 |  1.018604958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:15 | 200 |  1.530364208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:15 | 200 |  1.513209417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:17 | 200 |  256.077625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:19 | 200 |   208.01925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:22 | 200 |  872.515959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:22 | 200 |  866.909334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:26 | 200 |  258.686625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:26 | 200 |  176.974459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:27 | 200 |   543.02075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:27 | 200 |  1.032475708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:29 | 200 |  235.856959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:30 | 200 |     222.034ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:34 | 200 |  660.078875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:34 | 200 |  664.202292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:37 | 200 |  266.832125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:37 | 200 |  198.237291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:38 | 200 |  684.414292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:39 | 200 |   253.29325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:40 | 200 |  219.524583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:42 | 200 |  210.999084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:44 | 200 |     433.584µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 02:00:44 | 200 |   17.160208ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 02:00:46 | 200 |  1.969402292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:47 | 200 |  2.029995541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:47 | 200 |   2.02574275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:48 | 200 |  299.437459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:49 | 200 |   171.69775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:50 | 200 |     710.747ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:50 | 200 |  666.835667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:51 | 200 |  259.602708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:53 | 200 |  226.064708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:57 | 200 |      22.708µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 02:00:57 | 200 |   15.865625ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 02:00:59 | 200 |  1.988208166s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:59 | 200 |  1.408568375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:00:59 | 200 |  1.425505459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:00 | 200 |  347.709292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:00 | 200 |    150.7895ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:02 | 200 |  576.711834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:02 | 200 |  569.840334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:03 | 200 |  313.602959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:04 | 200 |  223.932959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:09 | 200 |     229.583µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 02:01:09 | 200 |   14.183375ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 02:01:11 | 200 |  1.919553959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:11 | 200 |  338.677583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:11 | 200 |  459.727375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:11 | 200 |  1.238971417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:12 | 200 |   1.23268575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:13 | 200 |  571.027375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:13 | 200 |  567.571542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:14 | 200 |  219.875708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:15 | 200 |     218.672ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:17 | 200 |       23.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 02:01:17 | 200 |   16.707042ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 02:01:18 | 200 |  1.653884042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:22 | 200 |  233.145833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:22 | 200 |    166.5615ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:23 | 200 |  1.064077333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:24 | 200 |   1.07026275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:24 | 200 |      27.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 02:01:24 | 200 |   32.103708ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 02:01:26 | 200 |  758.144958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:26 | 200 |     1.393778s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:26 | 200 |  1.779438416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:26 | 200 |  1.411945167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:27 | 200 |  178.193541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:34 | 200 |      229.94ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:34 | 200 |  198.563459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:35 | 200 |  392.502875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:35 | 200 |  628.513125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:37 | 200 |     220.216ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:37 | 200 |  713.934833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:38 | 200 |  701.714959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:38 | 200 |  157.146917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:45 | 200 |   223.37175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:45 | 200 |  205.896458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:46 | 200 |  460.248834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:47 | 200 |  228.465417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:48 | 200 |  278.664125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:49 | 200 |  548.258417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:49 | 200 |  257.065917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:49 | 200 |   534.00725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:56 | 200 |  213.205291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:56 | 200 |  217.450083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:58 | 200 |  485.802708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:58 | 200 |  478.343916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:01:59 | 200 |  229.214958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:00 | 200 |  175.225917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:01 | 200 |  849.400584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:01 | 200 |  865.866833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:07 | 200 |    250.0265ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:07 | 200 |  247.330791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:10 | 200 |  674.287584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:10 | 200 |  1.327086125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:11 | 200 |  229.914958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:12 | 200 |  355.078458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:12 | 200 |  463.131792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:13 | 200 |   462.04175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:19 | 200 |  242.760875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:19 | 200 |  239.613292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:21 | 200 |  768.477583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:22 | 200 |  389.949209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:22 | 200 |  834.287292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:23 | 200 |  169.426875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:24 | 200 |  601.033875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:25 | 200 |  1.163191458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:30 | 200 |  231.889333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:30 | 200 |  233.263125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:33 | 200 |  303.713666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:34 | 200 |  1.131981209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:34 | 200 |     413.897ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:34 | 200 |  154.400416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:36 | 200 |  482.529708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:36 | 200 |  289.996625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:41 | 200 |  219.790125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:41 | 200 |     221.826ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:45 | 200 |  220.687959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:45 | 200 |    555.8485ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:45 | 200 |  231.004083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:46 | 200 |  1.218031792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:47 | 200 |    833.0915ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:48 | 200 |  477.755584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:52 | 200 |  282.339458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:53 | 200 |    287.3495ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:56 | 200 |  235.232292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:57 | 200 |  242.864875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:57 | 200 |  436.483291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:57 | 200 |  269.078375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:59 | 200 |  518.330417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:02:59 | 200 |  520.938792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:04 | 200 |  268.934708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:04 | 200 |   264.95675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:07 | 200 |   230.62975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:08 | 200 |  195.820792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:08 | 200 |   657.99925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:09 | 200 |  218.871958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:10 | 200 |   499.85925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:11 | 200 |   501.51825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:15 | 200 |  263.554041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:15 | 200 |  268.496792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:18 | 200 |   223.27675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:19 | 200 |  174.093709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:20 | 200 |  1.131917875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:21 | 200 |  1.073491708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:22 | 200 |    738.1345ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:22 | 200 |  708.277292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:26 | 200 |  280.182834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:26 | 200 |     272.535ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:30 | 200 |  225.085417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:30 | 200 |  164.847708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:32 | 200 |  876.948083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:33 | 200 |  879.317708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:34 | 200 |  530.216708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:34 | 200 |  528.215875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:38 | 200 |     307.127ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:38 | 200 |  312.560917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:41 | 200 |  215.754334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:41 | 200 |  187.141583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:45 | 200 |  1.098516917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:45 | 200 |  1.340896292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:45 | 200 |    586.1525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:46 | 200 |  959.087667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:49 | 200 |  323.143375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:49 | 200 |  328.081917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:52 | 200 |   214.19875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:53 | 200 |     169.124ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:57 | 200 |    960.5275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:57 | 200 |  733.671875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:57 | 200 |  563.697334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:03:57 | 200 |  259.095875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:00 | 200 |  278.679667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:00 | 200 |  276.166041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:03 | 200 |  219.075083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:04 | 200 |  161.695333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:08 | 200 |  417.038167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:08 | 200 |  415.919541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:09 | 200 |  927.021291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:09 | 200 |   851.53025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:12 | 200 |  314.100375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:12 | 200 |  310.600041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:15 | 200 |    220.7705ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:15 | 200 |   141.66375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:20 | 200 |  645.428333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:20 | 200 |    585.8135ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:21 | 200 |  552.939916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:21 | 200 |  548.742041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:23 | 200 |  352.449875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:23 | 200 |    351.8735ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:26 | 200 |  229.875334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:26 | 200 |   143.57575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:31 | 200 |  389.845542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:32 | 200 |  850.286625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:32 | 200 |  551.747084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:32 | 200 |  534.700333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:34 | 200 |  328.134917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:35 | 200 |  323.798834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:37 | 200 |  224.501958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:37 | 200 |    191.4605ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:43 | 200 |  463.560792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:43 | 200 |  275.587958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:44 | 200 |   1.13355225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:45 | 200 |  1.156170125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:46 | 200 |  410.868125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:46 | 200 |  407.832416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:48 | 200 |  220.379417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:49 | 200 |     226.215ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:54 | 200 |    795.8115ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:55 | 200 |  521.353041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:56 | 200 |  1.045724416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:57 | 200 |  1.044392667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:57 | 200 |  496.737291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:04:58 | 200 |  496.537583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:00 | 200 |  236.597125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:00 | 200 |  232.247959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:06 | 200 |  410.777666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:07 | 200 |  1.067844458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:08 | 200 |  449.169334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:08 | 200 |  442.394666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:09 | 200 |  286.239542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:09 | 200 |  318.809125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:11 | 200 |   223.09025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:11 | 200 |  224.630208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:17 | 200 |  487.562458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:18 | 200 |  269.032042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:20 | 200 |  918.868833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:20 | 200 |  391.114917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:20 | 200 |   428.56025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:20 | 200 |   1.32014525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:22 | 200 |    219.8515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:22 | 200 |  219.681708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:29 | 200 |  447.544709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:30 | 200 |  769.584291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:32 | 200 |  346.635375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:32 | 200 |  333.938875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:32 | 200 |  1.027917708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:33 | 200 |  1.093349417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:33 | 200 |  216.743542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:34 | 200 |  216.287833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:40 | 200 |  395.181083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:42 | 200 |  744.289459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:43 | 200 |  240.869125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:43 | 200 |  156.576917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:43 | 200 |  432.137667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:44 | 200 |  275.256083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:45 | 200 |  215.745792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:45 | 200 |  230.885125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:52 | 200 |  490.459334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:53 | 200 |  242.810042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:54 | 200 |  312.431834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:54 | 200 |  307.860167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:55 | 200 |  555.669167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:55 | 200 |   262.30575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:56 | 200 |  222.170667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:05:56 | 200 |  224.953417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:03 | 200 |  506.657208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:05 | 200 |  808.223917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:06 | 200 |  323.946541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:06 | 200 |  329.778834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:06 | 200 |  494.130083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:07 | 200 |  886.900333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:07 | 200 |  179.838125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:07 | 200 |  179.987792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:15 | 200 |  402.471167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:16 | 200 |  531.895708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:17 | 200 |  218.991833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:17 | 200 |  227.943916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:18 | 200 |  425.947917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:18 | 200 |   296.48375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:18 | 200 |  216.265375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:19 | 200 |    217.4815ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:27 | 200 |  1.032690333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:28 | 200 |  269.355083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:28 | 200 |  266.625666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:28 | 200 |    257.4415ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:29 | 200 |  391.231542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:30 | 200 |  315.341292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:30 | 200 |  441.185875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:30 | 200 |  676.067916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:38 | 200 |  462.977416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:39 | 200 |  267.051667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:39 | 200 |  266.260542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:39 | 200 |  275.063042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:41 | 200 |  451.030541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:41 | 200 |  193.514916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:41 | 200 |  175.316792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:41 | 200 |  270.940875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:50 | 200 |  391.327791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:50 | 200 |  308.524333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:51 | 200 |   258.61775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:51 | 200 |  176.287417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:52 | 200 |  278.991917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:52 | 200 |  545.686458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:52 | 200 |  195.107125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:06:53 | 200 |  253.100375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:01 | 200 |  521.474458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:01 | 200 |  249.313417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:02 | 200 |  250.833583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:02 | 200 |  253.655666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:04 | 200 |  276.189042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:04 | 200 |  314.316166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:04 | 200 |  526.800042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:04 | 200 |     448.375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:13 | 200 |  311.352542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:13 | 200 |   402.35675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:14 | 200 |  1.742025209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:14 | 200 |   1.74715675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:15 | 200 |  258.630166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:15 | 200 |  223.481292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:16 | 200 |     837.916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:16 | 200 |   832.98125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:25 | 200 |  213.888292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:25 | 200 |  138.999334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:26 | 200 |   900.03875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:26 | 200 |  951.388292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:26 | 200 |  273.103125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:26 | 200 |  273.267292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:27 | 200 |  482.305417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:28 | 200 |  488.333625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:36 | 200 |  226.711166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:36 | 200 |  202.442708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:38 | 200 |  331.714875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:38 | 200 |  421.426875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:38 | 200 |  810.442834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:38 | 200 |  759.461708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:39 | 200 |     515.386ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:39 | 200 |  504.958125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:47 | 200 |  216.216334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:47 | 200 |  222.121375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:49 | 200 |  226.910458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:49 | 200 |  204.552542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:50 | 200 |  1.048364709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:50 | 200 |  1.064105875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:50 | 200 |  527.646375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:51 | 200 |  522.806666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:58 | 200 |  203.583208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:07:58 | 200 |  207.335208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:00 | 200 |     228.178ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:00 | 200 |  161.733375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:02 | 200 |  838.821083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:02 | 200 |   635.43475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:02 | 200 |  979.631209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:02 | 200 |  640.860333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:10 | 200 |  246.627292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:10 | 200 |  276.335875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:11 | 200 |  217.829416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:11 | 200 |   215.23075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:14 | 200 |  977.881333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:14 | 200 |  979.162834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:14 | 200 |    1.1812105s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:15 | 200 |  1.181826292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:21 | 200 |  311.380917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:21 | 200 |  280.842167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:23 | 200 |  246.234417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:23 | 200 |  258.248583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:25 | 200 |  458.365209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:25 | 200 |  321.970834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:26 | 200 |  520.257084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:26 | 200 |  522.635166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:32 | 200 |  299.679833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:32 | 200 |  321.311292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:34 | 200 |  257.420667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:34 | 200 |  274.466167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:37 | 200 |  552.004541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:37 | 200 |  551.116084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:37 | 200 |   576.25425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:38 | 200 |  609.721916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:44 | 200 |  345.251916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:44 | 200 |  349.208042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:45 | 200 |    226.6785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:45 | 200 |  210.335709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:48 | 200 |  551.450416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:49 | 200 |  523.569916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:49 | 200 |  652.664209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:49 | 200 |  619.981084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:55 | 200 |  386.869084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:55 | 200 |  357.469416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:56 | 200 |  244.031791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:08:57 | 200 |  236.355459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:00 | 200 |  580.259875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:00 | 200 |  559.810875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:01 | 200 |  1.035821083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:01 | 200 |  1.033615375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:06 | 200 |  301.366042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:06 | 200 |     294.471ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:08 | 200 |  199.998291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:08 | 200 |   198.18675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:11 | 200 |   426.05275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:12 | 200 |    883.4115ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:13 | 200 |  524.599334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:13 | 200 |    509.4565ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:18 | 200 |  325.068875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:18 | 200 |  326.686208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:19 | 200 |  177.737541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:19 | 200 |  202.104125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:23 | 200 |  389.695167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:24 | 200 |  519.441459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:24 | 200 |  563.834958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:25 | 200 |  570.429333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:29 | 200 |  335.280625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:29 | 200 |  331.452875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:30 | 200 |  177.262792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:30 | 200 |     155.812ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:34 | 200 |   322.48675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:35 | 200 |   263.52425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:36 | 200 |  883.705666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:36 | 200 |  858.296791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:40 | 200 |  295.413208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:40 | 200 |    291.5915ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:41 | 200 |  174.314667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:41 | 200 |  175.007208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:46 | 200 |  791.144041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:46 | 200 |  271.083625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:48 | 200 |  890.731791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:49 | 200 |  1.513610042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:52 | 200 |   340.26425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:52 | 200 |  330.854041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:53 | 200 |  217.037208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:53 | 200 |  219.275166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:58 | 200 |  1.125046041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:09:59 | 200 |     1.720175s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:00 | 200 |   703.67225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:00 | 200 |   295.95075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:03 | 200 |  421.341458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:03 | 200 |  417.064917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:04 | 200 |   170.55325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:04 | 200 |  164.679375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:10 | 200 |  569.190708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:10 | 200 |  371.259208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:12 | 200 |     690.189ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:12 | 200 |  470.183584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:15 | 200 |  333.126292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:15 | 200 |  336.040875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:15 | 200 |  176.693791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:15 | 200 |  189.121833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:22 | 200 |     945.889ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:22 | 200 |     517.657ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:23 | 200 |    537.5305ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:24 | 200 |    989.2755ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:26 | 200 |  275.234125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:26 | 200 |  273.147666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:26 | 200 |     152.642ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:26 | 200 |     152.401ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:33 | 200 |  316.458042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:33 | 200 |  322.880416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:35 | 200 |  405.077625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:35 | 200 |  279.871792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:37 | 200 |   383.87775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:37 | 200 |     390.402ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:37 | 200 |  180.276667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:38 | 200 |  223.091209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:45 | 200 |  592.678625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:45 | 200 |   592.68375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:46 | 200 |   595.28925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:47 | 200 |  798.678125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:49 | 200 |  302.136708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:49 | 200 |  300.915334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:49 | 200 |   289.80525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:49 | 200 |  298.950667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:56 | 200 |  310.841292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:56 | 200 |  308.996708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:58 | 200 |  491.237083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:10:59 | 200 |  629.947459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:00 | 200 |  306.210666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:00 | 200 |  314.419209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:00 | 200 |  308.284125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:00 | 200 |  312.210625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:08 | 200 |  437.655292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:08 | 200 |  449.323458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:09 | 200 |     444.416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:10 | 200 |  311.757042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:11 | 200 |  246.817791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:11 | 200 |  252.899834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:11 | 200 |  276.446541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:11 | 200 |  277.811541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:19 | 200 |  431.866666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:19 | 200 |  429.426125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:21 | 200 |  427.174541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:21 | 200 |  288.409167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:22 | 200 |  234.649791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:23 | 200 |  231.038917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:23 | 200 |  218.030125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:23 | 200 |    213.8355ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:30 | 200 |  346.087542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:31 | 200 |  575.920375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:32 | 200 |     429.964ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:33 | 200 |  277.761542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:34 | 200 |    246.8715ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:34 | 200 |  237.854666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:34 | 200 |  244.912375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:34 | 200 |  265.302167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:42 | 200 |  339.038208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:42 | 200 |    235.4775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:44 | 200 |  707.071583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:44 | 200 |  455.741875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:45 | 200 |  234.344625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:45 | 200 |  235.373916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:45 | 200 |    231.3245ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:45 | 200 |  216.566416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:53 | 200 |  340.778375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:53 | 200 |     220.436ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:55 | 200 |  364.586417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:55 | 200 |  369.292375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:56 | 200 |  231.313417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:56 | 200 |  236.889792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:56 | 200 |  235.855167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:11:57 | 200 |  236.314667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:05 | 200 |  1.025895834s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:06 | 200 |  1.683598459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:07 | 200 |  449.567833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:08 | 200 |  991.633166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:08 | 200 |  260.445791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:08 | 200 |  264.999333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:08 | 200 |  278.727291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:08 | 200 |  277.518333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:17 | 200 |  323.720083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:17 | 200 |   252.46925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:18 | 200 |  372.653208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:19 | 200 |  285.995208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:19 | 200 |  381.002125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:19 | 200 |      313.41ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:19 | 200 |   290.81475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:19 | 200 |  301.067209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:28 | 200 |  389.016167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:29 | 200 |  258.413708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:30 | 200 |  512.729416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:30 | 200 |  370.636917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:30 | 200 |  373.094833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:30 | 200 |  374.604166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:30 | 200 |  376.076583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:31 | 200 |  362.773834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:39 | 200 |  383.154375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:40 | 200 |  250.385416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |  295.097583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |  945.512667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |  349.427167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |  441.057458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |   666.00025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:42 | 200 |  477.618084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:51 | 200 |  1.036654834s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:52 | 200 |  727.544583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:53 | 200 |  289.917375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:53 | 200 |  168.012458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:53 | 200 |  244.937458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:53 | 200 |  663.097458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:53 | 200 |  371.482916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:12:54 | 200 |  574.022791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:03 | 200 |  465.723708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:03 | 200 |  449.997334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:04 | 200 |  295.323125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:04 | 200 |  298.780333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:04 | 200 |  145.985792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:05 | 200 |  221.502083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:05 | 200 |  456.404792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:05 | 200 |  419.549541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:14 | 200 |  336.086666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:14 | 200 |  335.980334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:15 | 200 |  231.558791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:16 | 200 |  227.387958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:16 | 200 |  176.684625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:16 | 200 |   158.25025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:16 | 200 |  352.068791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:16 | 200 |  363.367125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:26 | 200 |  386.690125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:26 | 200 |  916.413208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:27 | 200 |  207.612583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:27 | 200 |  201.642625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:27 | 200 |  202.978125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:27 | 200 |   142.95975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:28 | 200 |  467.802666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:28 | 200 |  594.481542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:37 | 200 |  334.754334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:38 | 200 |    295.8355ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:38 | 200 |  267.333542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:38 | 200 |  293.801959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:38 | 200 |  281.220959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:38 | 200 |  205.531708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:39 | 200 |  456.750667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:40 | 200 |  459.556708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:49 | 200 |  686.377541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:49 | 200 |  481.552791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:49 | 200 |  281.316416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:49 | 200 |  245.974625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:50 | 200 |   262.23975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:50 | 200 |  257.969916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:51 | 200 |  477.776542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:13:51 | 200 |  485.477958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:00 | 200 |  336.614917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:01 | 200 |  250.866333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:01 | 200 |    299.3335ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:01 | 200 |  324.686792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:01 | 200 |  334.619625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:01 | 200 |  337.813542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:02 | 200 |  467.715667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:03 | 200 |  470.316958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:12 | 200 |  649.364792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:12 | 200 |  320.358458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:12 | 200 |  406.209375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:12 | 200 |  481.441958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:13 | 200 |  958.122417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:13 | 200 |  557.891583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:14 | 200 |  433.382792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:14 | 200 |  433.000292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:23 | 200 |  345.076792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:23 | 200 |  231.708542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:23 | 200 |  140.882417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:24 | 200 |  139.811041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:24 | 200 |  240.347625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:24 | 200 |   284.69975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:25 | 200 |    409.8855ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:26 | 200 |  666.731667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  320.317916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  395.086333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  323.007958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  223.087625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  240.621333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:35 | 200 |  293.621625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:37 | 200 |   466.99625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:37 | 200 |  256.219334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:46 | 200 |  371.903875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:46 | 200 |     517.729ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:46 | 200 |  495.416167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:46 | 200 |  595.938375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:47 | 200 |  327.144209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:47 | 200 |  460.031625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:48 | 200 |  478.032875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:48 | 200 |  439.803833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:57 | 200 |  277.323375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:57 | 200 |  147.839708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:58 | 200 |  243.005125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:58 | 200 |  624.611334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:58 | 200 |  295.216583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:14:58 | 200 |  522.870916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:00 | 200 |  448.248459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:00 | 200 |  450.094375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |  223.465333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |  242.627834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |  150.910708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |  159.761959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |  407.699583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:09 | 200 |    381.6705ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:11 | 200 |  480.372166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:11 | 200 |  483.415458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:20 | 200 |  236.601917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:20 | 200 |  236.596916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:20 | 200 |  141.142833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:20 | 200 |   150.97025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:21 | 200 |  398.585833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:21 | 200 |  885.250958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:23 | 200 |  425.577625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:23 | 200 |  426.857625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:31 | 200 |  238.690167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:31 | 200 |  238.764166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:31 | 200 |   226.47725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:31 | 200 |  139.978709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:32 | 200 |     571.807ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:33 | 200 |  244.324959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:34 | 200 |  381.356167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:34 | 200 |  380.378667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:42 | 200 |  221.884042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:42 | 200 |  221.091875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:43 | 200 |  221.858833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:43 | 200 |  200.058959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:44 | 200 |  369.786375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:44 | 200 |  214.014417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:46 | 200 |  397.197417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:46 | 200 |     406.405ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:54 | 200 |  226.057708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:54 | 200 |  231.063333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:54 | 200 |  230.597083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:54 | 200 |  231.580042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:56 | 200 |  1.220883292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:56 | 200 |  1.213929042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:57 | 200 |  419.393292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:15:57 | 200 |  412.894459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:05 | 200 |  223.271125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:05 | 200 |  220.689209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:05 | 200 |   219.98275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:05 | 200 |  220.207458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:08 | 200 |     906.199ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:08 | 200 |  914.220334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:08 | 200 |   357.39325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:09 | 200 |  369.374708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:16 | 200 |   241.79175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:16 | 200 |  243.620875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:16 | 200 |  245.387625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:16 | 200 |  245.999542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:20 | 200 |  551.794791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:20 | 200 |    1.1790245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:20 | 200 |  743.321584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:20 | 200 |  1.286982959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:27 | 200 |  214.922042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:27 | 200 |  213.280125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:28 | 200 |  210.411208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:28 | 200 |   175.28475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:31 | 200 |    471.9295ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:32 | 200 |  455.564792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:32 | 200 |  480.804084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:32 | 200 |     463.912ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:39 | 200 |  216.411292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:39 | 200 |  218.257375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:39 | 200 |  212.810791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:39 | 200 |  215.259333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:43 | 200 |  453.259625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:43 | 200 |  482.508292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:44 | 200 |  1.184198416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:44 | 200 |     997.332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:50 | 200 |  221.806667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:50 | 200 |   222.91075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:50 | 200 |  221.919958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:50 | 200 |  220.298708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:55 | 200 |  792.724208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:55 | 200 |  804.668458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:55 | 200 |  541.595083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:16:55 | 200 |  549.637041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:01 | 200 |  208.933584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:01 | 200 |  209.095416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:01 | 200 |   210.05425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:01 | 200 |  210.485833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:06 | 200 |  315.607167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:07 | 200 |  273.003125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:07 | 200 |  404.204917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:07 | 200 |   399.87925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:12 | 200 |  213.510875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:12 | 200 |  244.369334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:13 | 200 |   236.79125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:13 | 200 |    235.0555ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:18 | 200 |  320.112875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:18 | 200 |  228.919625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:18 | 200 |  341.325792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:18 | 200 |  346.385291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:24 | 200 |  209.635334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:24 | 200 |  189.459208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:24 | 200 |  205.750458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:24 | 200 |  210.002709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:29 | 200 |  306.473042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:29 | 200 |  299.897375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:30 | 200 |  751.624958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:30 | 200 |  749.876792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:35 | 200 |  211.160459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:35 | 200 |  212.942625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:35 | 200 |  191.999167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:35 | 200 |   183.61175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:40 | 200 |  312.570833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:41 | 200 |   532.84625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:41 | 200 |   377.86975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:41 | 200 |  368.027666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:46 | 200 |    207.6135ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:46 | 200 |  210.099583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:46 | 200 |  207.611917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:46 | 200 |  209.571584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:52 | 200 |  310.864292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:53 | 200 |  1.033361792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:54 | 200 |  1.935355416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:54 | 200 |    1.1984995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:57 | 200 |  215.641875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:57 | 200 |   222.32825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:57 | 200 |  222.633791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:17:58 | 200 |  226.148791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:04 | 200 |  917.695959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:05 | 200 |     362.377ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:05 | 200 |   437.21375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:06 | 200 |  888.915584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:09 | 200 |  219.967875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:09 | 200 |  220.094542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:09 | 200 |  220.859125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:09 | 200 |  222.615542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:15 | 200 |  306.120875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:16 | 200 |  380.522416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:16 | 200 |  296.347959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:18 | 200 |  939.515584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:20 | 200 |  240.957917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:20 | 200 |  251.644917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:20 | 200 |  251.457208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:20 | 200 |  256.260625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:26 | 200 |  317.295833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:27 | 200 |   362.61075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:28 | 200 |  570.154334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:29 | 200 |  372.393083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:31 | 200 |  216.934791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:31 | 200 |  221.787958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:31 | 200 |  219.711458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:31 | 200 |  220.774708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:38 | 200 |  314.795417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:39 | 200 |   398.00425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:39 | 200 |  311.474083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:40 | 200 |   275.83725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:42 | 200 |  262.243917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:42 | 200 |  237.733375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:43 | 200 |  283.092583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:43 | 200 |     277.793ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:49 | 200 |  832.532208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:50 | 200 |  416.732958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:51 | 200 |  216.969334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:52 | 200 |     283.678ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:54 | 200 |  236.540084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:54 | 200 |  216.463042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:54 | 200 |  401.747667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:18:54 | 200 |  403.397125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:01 | 200 |  329.960083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:02 | 200 |  534.788333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:03 | 200 |  949.998417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:03 | 200 |  252.614583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:05 | 200 |  245.360917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:05 | 200 |  258.084625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:05 | 200 |  138.471542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:05 | 200 |      141.59ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:13 | 200 |  944.677709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:13 | 200 |  356.658375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:14 | 200 |  455.350291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:15 | 200 |  1.062400708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:16 | 200 |  217.262417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:16 | 200 |  183.183708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:16 | 200 |  141.051792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:16 | 200 |  142.060208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:24 | 200 |  312.332959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:25 | 200 |  683.684083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:26 | 200 |  435.954583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:26 | 200 |  947.943125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:27 | 200 |  207.729208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:27 | 200 |    211.2395ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:28 | 200 |  194.598458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:28 | 200 |  192.503042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:35 | 200 |  317.749875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:36 | 200 |  346.580625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:38 | 200 |  398.657209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:38 | 200 |  758.240459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:39 | 200 |  243.089208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:39 | 200 |  245.903291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:39 | 200 |  243.176542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:39 | 200 |  264.297542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:47 | 200 |   893.39425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:48 | 200 |  331.352667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:49 | 200 |  306.374209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:49 | 200 |  344.701667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:50 | 200 |  209.900792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:50 | 200 |  229.716667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:50 | 200 |   228.34825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:50 | 200 |  211.798291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:19:59 | 200 |  991.010416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:00 | 200 |  878.028208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:00 | 200 |  484.671416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:01 | 200 |  520.619084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:01 | 200 |  195.493417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:01 | 200 |  178.086459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:01 | 200 |   182.07875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:01 | 200 |  182.685375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:11 | 200 |    320.6715ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:11 | 200 |  671.168167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:12 | 200 |  465.278916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:12 | 200 |  314.707333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:12 | 200 |  226.892167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:12 | 200 |     240.623ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:13 | 200 |   242.88975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:13 | 200 |  242.660083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:23 | 200 |  814.544917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:23 | 200 |  665.426042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:23 | 200 |   257.67325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:24 | 200 |  298.460208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:24 | 200 |  468.272708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:24 | 200 |  444.681708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:24 | 200 |  479.270208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:24 | 200 |   483.25775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:34 | 200 |  324.213416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  352.827125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  779.193833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  347.660209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  242.198625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  242.417333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  230.846166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:35 | 200 |  238.175416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:46 | 200 |  745.895709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:46 | 200 |  284.370792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:46 | 200 |  318.780167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:46 | 200 |  364.346667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:46 | 200 |  251.553583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:47 | 200 |  248.887458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:47 | 200 |  231.525875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:47 | 200 |   247.35925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:57 | 200 |  309.318333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  547.217333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  471.475458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  1.193081042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  619.806709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  745.229958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |   622.96275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:20:58 | 200 |  604.284333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:08 | 200 |      307.28ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:09 | 200 |  423.336084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:09 | 200 |  317.149208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:10 | 200 |  376.453541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:10 | 200 |     472.684ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:10 | 200 |  485.582125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:10 | 200 |    435.1375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:10 | 200 |  434.675417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:20 | 200 |  306.560458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |  262.682167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |  435.056666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |  290.291542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |  442.270542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |   384.35725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |   470.04125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:21 | 200 |  415.574875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:31 | 200 |     312.768ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:32 | 200 |  211.291875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:32 | 200 |  372.223792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:32 | 200 |   199.66575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:32 | 200 |     336.781ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:32 | 200 |  298.748042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:33 | 200 |  416.802041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:33 | 200 |  380.143667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:42 | 200 |  308.642166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:43 | 200 |  212.865459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:43 | 200 |  196.952167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:43 | 200 |  372.106709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:44 | 200 |    238.5235ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:44 | 200 |   369.50425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:44 | 200 |  257.994833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:44 | 200 |  352.763292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:54 | 200 |  513.972167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:54 | 200 |  209.254542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |  151.752875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |  334.692792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |   256.96125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |  343.446625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |  268.318417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:21:55 | 200 |    319.0055ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:05 | 200 |  306.410834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:06 | 200 |  220.910291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:06 | 200 |  145.915416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:06 | 200 |    237.4255ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:06 | 200 |   451.95875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:07 | 200 |  266.853041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:07 | 200 |   382.98625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:07 | 200 |  334.239625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:17 | 200 |  301.905334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:17 | 200 |  187.145125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:17 | 200 |  139.652334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:17 | 200 |  211.018458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:18 | 200 |  423.692708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:18 | 200 |  301.183209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:18 | 200 |    405.4765ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:18 | 200 |  356.987458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:28 | 200 |  339.102791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:28 | 200 |  220.292833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:28 | 200 |     202.022ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:29 | 200 |  169.993542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:29 | 200 |     226.703ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:30 | 200 |  485.191708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:30 | 200 |     1.331364s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:30 | 200 |  1.198398792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:39 | 200 |  281.041458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:39 | 200 |  382.453375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:40 | 200 |  474.538167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:40 | 200 |   168.57325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:40 | 200 |    184.3015ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:42 | 200 |  508.583042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:42 | 200 |  1.127879125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:42 | 200 |  507.675042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:51 | 200 |  303.886166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:51 | 200 |   379.65225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:51 | 200 |  181.219542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:51 | 200 |  163.946625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:51 | 200 |  162.341375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:53 | 200 |  448.981459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:53 | 200 |  511.630292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:22:54 | 200 |  1.103765792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:02 | 200 |  236.914792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:02 | 200 |  265.600375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:02 | 200 |  234.272875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:03 | 200 |  230.684541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:03 | 200 |  1.086669584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:05 | 200 |  443.573542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:05 | 200 |  482.824292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:05 | 200 |  284.289417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:13 | 200 |  202.990333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:13 | 200 |   137.05975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:14 | 200 |   161.92925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:14 | 200 |  164.990625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:14 | 200 |  284.330083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:16 | 200 |  417.884583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:16 | 200 |  725.507292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:17 | 200 |  301.408084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:24 | 200 |  206.480958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:25 | 200 |   354.56975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:25 | 200 |  152.487833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:25 | 200 |  164.970209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:26 | 200 |  894.511625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:27 | 200 |  237.673417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:28 | 200 |     361.776ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:28 | 200 |  362.818917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:36 | 200 |  225.567833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:36 | 200 |    145.4365ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:36 | 200 |  145.368125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:36 | 200 |  169.623084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:37 | 200 |  300.871333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:39 | 200 |  248.255125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:39 | 200 |  352.258708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:39 | 200 |  355.747792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:47 | 200 |  229.146083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:47 | 200 |  139.055917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:47 | 200 |  142.651041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:48 | 200 |  161.936208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:49 | 200 |  302.987291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:50 | 200 |  238.236458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:51 | 200 |  349.965292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:51 | 200 |  879.191667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:58 | 200 |  205.823333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:58 | 200 |   205.72175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:58 | 200 |  202.196416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:23:59 | 200 |  168.644583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:00 | 200 |  305.216042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:01 | 200 |  243.577083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:02 | 200 |  351.071333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:03 | 200 |  592.085625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:09 | 200 |  219.452084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:09 | 200 |  220.665291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:10 | 200 |  223.821625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:10 | 200 |  173.258916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:12 | 200 |  964.689958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:12 | 200 |  248.025958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:13 | 200 |  346.059042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:15 | 200 |   842.29825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:21 | 200 |  208.508875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:21 | 200 |  202.361375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:21 | 200 |  205.155209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:21 | 200 |  166.001458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:24 | 200 |     534.302ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:24 | 200 |  1.007989458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:25 | 200 |  657.008458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:27 | 200 |  795.737333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:32 | 200 |  202.540208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:32 | 200 |  208.087583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:32 | 200 |  207.216083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:32 | 200 |  162.171125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:35 | 200 |  529.035333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:36 | 200 |  816.486084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:36 | 200 |  404.476667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:38 | 200 |  359.605583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:43 | 200 |  205.493125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:43 | 200 |  206.982541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:43 | 200 |  225.662667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:44 | 200 |  165.392666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:46 | 200 |  310.155375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:48 | 200 |  244.537291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:48 | 200 |  323.042916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:50 | 200 |     601.076ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:54 | 200 |  204.770833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:54 | 200 |  210.881083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:54 | 200 |  189.840916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:55 | 200 |  174.707959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:58 | 200 |   304.87025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:59 | 200 |  239.359459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:24:59 | 200 |  349.604917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:01 | 200 |  353.758541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:06 | 200 |  204.530417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:06 | 200 |  227.432375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:06 | 200 |  228.190875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:06 | 200 |  147.315875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:09 | 200 |  520.116042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:10 | 200 |  257.493833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:11 | 200 |  347.377875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:12 | 200 |  348.925083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:17 | 200 |  208.726542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:17 | 200 |     210.046ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:17 | 200 |  219.420166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:17 | 200 |  138.498625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:21 | 200 |    313.6705ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:21 | 200 |  267.289667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:22 | 200 |  355.381916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:24 | 200 |  607.653333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:28 | 200 |  212.153209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:28 | 200 |  198.668584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:28 | 200 |  202.437042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:28 | 200 |  178.622708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:32 | 200 |  311.827917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:33 | 200 |  243.219125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:34 | 200 |  614.576417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:35 | 500 |  270.569708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:39 | 200 |  209.646666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:39 | 200 |  144.213458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:45 | 200 |  1.545995958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:45 | 200 |  635.442375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:51 | 200 |  228.393166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:51 | 200 |     394.029ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:56 | 200 |  318.000167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:25:57 | 200 |  1.030708458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:02 | 200 |  209.365166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:02 | 200 |  150.306958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:07 | 200 |  310.362583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:09 | 200 |  887.692417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:13 | 200 |  209.548959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:13 | 200 |  148.153333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:19 | 200 |  307.155084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:21 | 200 |  360.437292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:24 | 200 |    202.2405ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:24 | 200 |   138.25775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:30 | 200 |  303.835625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:32 | 200 |  348.586583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:35 | 200 |  203.932542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:36 | 200 |  206.153083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:42 | 200 |  503.979583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:44 | 200 |  744.259792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:47 | 200 |  204.921333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:47 | 200 |  207.187209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:53 | 200 |  308.297666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:55 | 200 |  354.017875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:58 | 200 |  206.896208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:26:58 | 200 |  212.330375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:04 | 200 |  306.315667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:07 | 200 |  586.144458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:09 | 200 |  212.275583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:09 | 200 |  213.708125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:16 | 200 |  307.802125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:19 | 200 |  1.131063542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:20 | 200 |  224.046167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:21 | 200 |  243.566292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:27 | 200 |  322.602667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:31 | 200 |  1.134643084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:32 | 200 |  204.267709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:32 | 200 |  181.941041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:38 | 200 |   316.81725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:42 | 200 |  373.412125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:43 | 200 |  270.015625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:43 | 200 |  303.810333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:50 | 200 |  860.274792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:54 | 200 |   356.53075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:54 | 200 |    219.6365ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:27:54 | 200 |  193.924833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:01 | 200 |  341.192625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:05 | 200 |     368.746ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:05 | 200 |  204.968417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:06 | 200 |  206.412458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:13 | 200 |     575.276ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:17 | 200 |  362.306375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:17 | 200 |  204.421709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:17 | 200 |  201.750209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:24 | 200 |  308.316542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:28 | 200 |    257.7945ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:28 | 200 |  335.699083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:28 | 200 |  809.372667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:36 | 200 |  312.346292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:39 | 200 |     204.132ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:39 | 200 |  134.039542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:40 | 200 |  350.635417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:47 | 200 |  318.694042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:50 | 200 |  204.537833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:51 | 200 |   210.12375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:52 | 200 |  1.018103208s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:28:58 | 200 |     315.964ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:02 | 200 |  204.844667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:02 | 200 |     207.546ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:03 | 200 |  373.964334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:10 | 200 |  300.638917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:13 | 200 |  218.121459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:13 | 200 |  224.812458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:15 | 200 |  357.467208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:21 | 200 |  315.798459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:24 | 200 |     211.651ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:24 | 200 |  211.164083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:26 | 200 |  360.670083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:33 | 200 |  873.785167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:35 | 200 |  213.424791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:35 | 200 |  214.016875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:38 | 200 |  728.478875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:44 | 200 |   304.64575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:47 | 200 |  210.456083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:47 | 200 |  212.164625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:49 | 200 |  358.274708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:56 | 200 |  304.212459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:58 | 200 |  226.199041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:29:58 | 200 |  224.080709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:01 | 200 |  680.637375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:07 | 200 |  527.658042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:09 | 200 |   223.40875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:09 | 200 |  229.468958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:13 | 200 |  746.462041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:19 | 200 |     875.666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:20 | 200 |  210.009875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:20 | 200 |  220.789833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:24 | 200 |    364.1315ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:31 | 200 |    535.5105ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:32 | 200 |  208.052666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:32 | 200 |  206.689958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:35 | 200 |  365.508458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:42 | 200 |  305.314792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:43 | 200 |   216.82325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:43 | 200 |  224.196167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:47 | 200 |  860.502666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:54 | 200 |  511.699458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:54 | 200 |  215.749209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:54 | 200 |  221.290083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:30:59 | 200 |  360.693167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:05 | 200 |  303.833667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:05 | 200 |  212.663083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:05 | 200 |  215.459333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:10 | 200 |  374.953875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:16 | 200 |  308.201875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:17 | 200 |     215.136ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:17 | 200 |  217.730166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:22 | 200 |  374.100916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:28 | 200 |  306.163084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:28 | 200 |  185.053375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:28 | 200 |  185.255166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:33 | 200 |  368.379042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:39 | 200 |  309.005833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:39 | 200 |  191.288958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:39 | 200 |  190.957958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:44 | 200 |  364.461708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:50 | 200 |  331.999666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:51 | 200 |  476.386042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:51 | 200 |  949.140583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:31:56 | 200 |  368.957084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:02 | 200 |  203.136667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:02 | 200 |  148.976459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:03 | 200 |  925.849667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:07 | 200 |  358.572041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:13 | 200 |   208.61675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:13 | 200 |  139.751792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:14 | 200 |  309.079375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:19 | 200 |  359.627125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:24 | 200 |  385.950292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:24 | 200 |  364.739792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:26 | 200 |  305.239292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:30 | 200 |  371.699375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:35 | 200 |  203.593083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:36 | 200 |  200.159625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:38 | 200 |  978.743459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:41 | 200 |  534.620458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:47 | 200 |  225.292334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:47 | 200 |  228.657875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:50 | 200 |  981.676875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:53 | 200 |  346.804541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:58 | 200 |  207.613875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:32:58 | 200 |  201.969417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:01 | 200 |  759.322167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:04 | 200 |  353.761167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:09 | 200 |  201.608458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:09 | 200 |  201.782833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:13 | 200 |  824.865375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:16 | 200 |  585.661875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:20 | 200 |  210.436792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:21 | 200 |  213.469708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:25 | 200 |  310.813792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:27 | 200 |  365.630875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:32 | 200 |  206.406125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:32 | 200 |  208.991958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:36 | 200 |  564.753625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:39 | 200 |   751.31725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:43 | 200 |   234.93925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:43 | 200 |  239.755875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:48 | 200 |  544.232083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:50 | 200 |  379.454375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:54 | 200 |  210.562417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:54 | 200 |  210.094833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:33:59 | 200 |  336.176667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:02 | 200 |  380.138083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:05 | 200 |  223.354625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:06 | 200 |   231.26975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:11 | 200 |  563.407125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:13 | 200 |  365.138709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:17 | 200 |   214.71925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:17 | 200 |  214.081584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:22 | 200 |  323.138667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:25 | 200 |  411.269958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:28 | 200 |  260.269333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:28 | 200 |    264.5325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:33 | 200 |  331.086375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:36 | 200 |  377.696334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:39 | 200 |  214.397959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:39 | 200 |  214.375709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:45 | 200 |  343.619291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:47 | 200 |  389.766917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:50 | 200 |  219.361333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:51 | 200 |  218.936958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:57 | 200 |  957.839667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:34:59 | 200 |  860.477292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:02 | 200 |  215.124541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:02 | 200 |  222.963417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:09 | 200 |  868.703834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:11 | 200 |  360.273416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:13 | 200 |  214.932541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:13 | 200 |  214.021167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:20 | 200 |  540.064167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:22 | 200 |  357.826875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:24 | 200 |  211.411875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:24 | 200 |  215.182542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:32 | 200 |  926.436125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:34 | 200 |  464.425666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:36 | 200 |  270.391833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:36 | 200 |  277.279292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:44 | 200 |    554.5865ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:45 | 200 |  424.902458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:47 | 200 |  219.572958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:47 | 200 |  221.418083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:55 | 200 |  768.813167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:56 | 200 |  355.729917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:58 | 200 |  217.388542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:35:58 | 200 |  227.855541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:07 | 200 |     562.405ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:09 | 200 |     1.020868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:09 | 200 |  218.221583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:09 | 200 |   222.06925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:19 | 200 |  977.286791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:20 | 200 |     353.233ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:21 | 200 |  221.369916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:21 | 200 |  220.445084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:31 | 200 |  948.879417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:31 | 200 |  334.325542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:32 | 200 |  249.667166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:32 | 200 |   271.27175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:42 | 200 |  310.147917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:43 | 200 |  332.011459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:43 | 200 |  246.042375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:43 | 200 |  227.981792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:54 | 200 |   536.64975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:54 | 200 |  297.012417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:55 | 200 |  405.213417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:36:55 | 200 |  1.280938875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:06 | 200 |  424.588583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:06 | 200 |    249.2325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:06 | 200 |  145.342375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:06 | 200 |  281.202125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:17 | 200 |  302.058166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:17 | 200 |  424.522458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:18 | 200 |  474.277666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:18 | 200 |  962.448083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:28 | 200 |  225.026541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:28 | 200 |  140.490708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:29 | 200 |  513.369667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:30 | 200 |  1.042151792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:40 | 200 |  228.163375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:40 | 200 |  205.620875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:41 | 200 |  376.055125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:41 | 200 |   284.89625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:51 | 200 |    229.8235ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:51 | 200 |  263.471292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:52 | 200 |   441.20425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:37:53 | 200 |  1.155085125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:02 | 200 |  223.168792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:02 | 200 |  215.544917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:04 | 200 |  321.208041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:04 | 200 |  355.709542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:13 | 200 |  230.127208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:13 | 200 |  224.447833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:15 | 200 |  324.780375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:16 | 200 |  358.111333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:25 | 200 |  226.212958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:25 | 200 |    230.5725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:27 | 200 |  515.053125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:27 | 200 |  1.089718708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:36 | 200 |  225.431542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:36 | 200 |    234.5785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:39 | 200 |   425.05275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:39 | 200 |   819.37625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:47 | 200 |  226.422708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:47 | 200 |  233.994875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:50 | 200 |  331.160166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:51 | 200 |  618.956917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:58 | 200 |  229.903584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:38:59 | 200 |  244.532959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:02 | 200 |  321.492666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:02 | 200 |  378.085667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:10 | 200 |  238.674333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:10 | 200 |  247.234916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:13 | 200 |  323.216583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:14 | 200 |  380.497167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:21 | 200 |  236.495334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:21 | 200 |  244.339834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:24 | 200 |  330.060625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:25 | 200 |  371.188959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:32 | 200 |  217.069541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:32 | 200 |  212.583417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:36 | 200 |  354.017125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:36 | 200 |   371.01475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:44 | 200 |  219.271042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:44 | 200 |  224.944083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:47 | 200 |  316.929709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:48 | 200 |  868.912458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:55 | 200 |  231.082084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:55 | 200 |   231.96475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:39:58 | 200 |  322.996208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:00 | 200 |  1.048812709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:06 | 200 |   245.44125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:06 | 200 |  244.932458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:10 | 200 |  314.028167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:12 | 200 |  386.019458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:17 | 200 |  239.398167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:17 | 200 |  248.832333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:21 | 200 |  334.941875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:23 | 200 |  376.779208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:29 | 200 |  231.221792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:29 | 200 |  233.967292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:32 | 200 |  342.401708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:35 | 200 |  370.760333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:40 | 200 |  219.476292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:40 | 200 |  221.377208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:44 | 200 |  342.085958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:46 | 200 |  421.656334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:51 | 200 |  234.019667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:51 | 200 |  258.569959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:56 | 200 |   624.96025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:40:58 | 200 |  382.648208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:02 | 200 |  248.949042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:03 | 200 |  256.583375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:07 | 200 |  396.617125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:09 | 200 |  417.364292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:14 | 200 |   227.84525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:14 | 200 |  232.109834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:18 | 200 |  351.201625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:20 | 200 |  359.504125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:25 | 200 |  232.140417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:25 | 200 |  214.942875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:30 | 200 |  343.627208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:32 | 200 |  772.523917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:36 | 200 |  228.931459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:36 | 200 |  236.279333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:41 | 200 |  355.307041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:44 | 200 |  368.624583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:48 | 200 |  231.058875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:48 | 200 |  240.442167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:53 | 200 |  619.489542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:56 | 200 |  874.595875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:59 | 200 |    354.1955ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:41:59 | 200 |  357.503083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:05 | 200 |  694.113625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:07 | 200 |  425.565541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:10 | 200 |   223.13975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:10 | 200 |  219.320417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:16 | 200 |    332.1475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:18 | 200 |  368.177959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:21 | 200 |     215.787ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:22 | 200 |  240.440625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:28 | 200 |  846.340833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:30 | 200 |  367.588041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:33 | 200 |  219.674375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:33 | 200 |  203.617208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:39 | 200 |  340.216125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:41 | 200 |  373.637708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:44 | 200 |    230.7205ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:44 | 200 |  236.887791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:51 | 200 |  329.543542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:53 | 200 |     370.877ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:55 | 200 |  224.579167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:42:55 | 200 |  230.949584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:03 | 200 |  932.216625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:04 | 200 |  643.573584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:07 | 200 |  237.539375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:07 | 200 |  242.947875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:15 | 200 |  998.649958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:16 | 200 |  374.944416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:18 | 200 |  237.616166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:18 | 200 |   266.64325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:26 | 200 |  350.774375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:27 | 200 |  370.761708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:29 | 200 |    234.3845ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:29 | 200 |  216.792667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:37 | 200 |  345.587167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:39 | 200 |  382.289416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:40 | 200 |  259.469667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:40 | 200 |  274.140333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:49 | 200 |  316.728708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:50 | 200 |  391.637042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:52 | 200 |  242.188917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:43:52 | 200 |   245.67175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:00 | 200 |  550.778166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:01 | 200 |  391.953917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:03 | 200 |  220.532792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:03 | 200 |  230.580833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:12 | 200 |  640.583875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:13 | 200 |   423.52875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:14 | 200 |  230.422334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:14 | 200 |  218.549375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:23 | 200 |   332.86025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:25 | 200 |  1.220936708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:25 | 200 |  225.716417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:26 | 200 |  230.555959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:35 | 200 |  337.720667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:37 | 200 |  366.143791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:37 | 200 |  196.108166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:37 | 200 |  203.483125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:46 | 200 |  342.758041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:48 | 200 |  265.036333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:48 | 200 |  465.392084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:48 | 200 |  305.411125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:57 | 200 |  332.612959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:44:59 | 200 |  268.262458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:00 | 200 |  450.991667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:00 | 200 |  553.288292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:09 | 200 |  838.247958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:11 | 200 |  249.384667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:11 | 200 |  217.836625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:11 | 200 |   722.23675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:21 | 200 |  953.147625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:22 | 200 |  205.503417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:22 | 200 |  190.021542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:23 | 200 |  383.917917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:33 | 200 |  560.735917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:33 | 200 |  185.395708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:33 | 200 |     189.952ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:35 | 200 |  756.233292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:44 | 200 |  274.468333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:44 | 200 |   419.53025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:45 | 200 |  180.460084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:46 | 200 |  362.844083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:56 | 200 |  311.797041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:56 | 200 |  393.589333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:56 | 200 |     205.267ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:45:57 | 200 |  372.294375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:07 | 200 |  235.146125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:07 | 200 |  260.042584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:07 | 200 |  358.479458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:09 | 200 |  716.843125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:18 | 200 |  219.322709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:18 | 200 |  139.761709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:19 | 200 |  286.592584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:21 | 200 |  372.223625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:30 | 200 |  255.114208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:30 | 200 |  283.776417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:30 | 200 |   291.06975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:32 | 200 |  370.208709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:41 | 200 |  254.340708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:41 | 200 |   237.40025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:41 | 200 |  297.776083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:44 | 200 |  757.221167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:52 | 200 |  222.780667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:52 | 200 |  243.334584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:53 | 200 |    343.9965ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:46:55 | 200 |  370.296833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:03 | 200 |  249.677542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:04 | 200 |  243.776208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:05 | 200 |   903.68575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:07 | 200 |  376.279417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:15 | 200 |  226.812042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:15 | 200 |     235.535ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:16 | 200 |  330.566458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:18 | 200 |  377.963625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:26 | 200 |  223.207125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:26 | 200 |  230.347292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:27 | 200 |  326.599917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:29 | 200 |  380.921333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:37 | 200 |  206.302083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:37 | 200 |  204.693667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:39 | 200 |  760.402792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:41 | 200 |  359.476708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:48 | 200 |  210.852584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:49 | 200 |  209.999167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:50 | 200 |  313.685791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:47:52 | 200 |    359.6655ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:00 | 200 |  231.309583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:00 | 200 |  236.394666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:02 | 200 |  804.250709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:04 | 200 |  371.570625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:11 | 200 |   232.03725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:11 | 200 |  239.075375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:14 | 200 |  345.693542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:15 | 200 |  784.717541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:22 | 200 |  245.935667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:22 | 200 |  245.196208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:25 | 200 |     329.384ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:27 | 200 |  358.720541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:34 | 200 |  429.752333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:34 | 200 |   440.78125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:36 | 200 |  333.925125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:38 | 200 |  367.616333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:45 | 200 |  253.862375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:45 | 200 |  264.043625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:48 | 200 |  723.470584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:50 | 200 |  901.413583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:56 | 200 |     228.613ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:48:56 | 200 |  232.316167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:00 | 200 |  1.198429917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:02 | 200 |   377.72075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:08 | 200 |  229.367708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:08 | 200 |  230.491959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:12 | 200 |  313.958791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:13 | 200 |    371.3295ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:19 | 200 |  235.987834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:19 | 200 |  253.633583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:23 | 200 |   332.08125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:25 | 200 |  746.682167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:30 | 200 |   231.99875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:30 | 200 |  244.636291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:35 | 200 |  337.814042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:37 | 200 |   782.35225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:41 | 200 |  225.855416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:41 | 200 |  232.591583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:46 | 200 |  344.372667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:48 | 200 |  369.784458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:53 | 200 |  226.560917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:53 | 200 |  235.228375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:57 | 200 |  341.111375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:49:59 | 200 |     362.392ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:04 | 200 |  220.524208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:04 | 200 |  228.298125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:09 | 200 |  338.314417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:11 | 200 |  370.640375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:15 | 200 |  237.184833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:15 | 200 |  253.023416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:20 | 200 |  335.048375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:23 | 200 |  1.169764458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:26 | 200 |  250.350209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:27 | 200 |     266.124ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:31 | 200 |  315.030875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:35 | 200 |  368.716791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:38 | 200 |  226.252167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:38 | 200 |  228.328791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:43 | 200 |  334.764041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:46 | 200 |  364.967458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:49 | 200 |   253.17725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:49 | 200 |  266.870042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:54 | 200 |     569.922ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:50:58 | 200 |  603.974125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:00 | 200 |  220.821459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:00 | 200 |  230.673417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:06 | 200 |  861.893542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:09 | 200 |    361.3515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:12 | 200 |  225.792958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:12 | 200 |  222.049291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:18 | 200 |  338.686625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:20 | 200 |  432.721417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:23 | 200 |  244.247375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:23 | 200 |  244.657959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:29 | 200 |  356.681584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:32 | 200 |  380.247167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:34 | 200 |  231.725792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:34 | 200 |  242.633459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:41 | 200 |  328.202125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:43 | 200 |    366.4055ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:45 | 200 |  226.894125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:45 | 200 |  244.993167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:53 | 200 |  1.021642666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:55 | 200 |     809.553ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:57 | 200 |  228.183625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:51:57 | 200 |  225.968833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:04 | 200 |  323.856917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:07 | 200 |     373.062ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:08 | 200 |    229.6875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:08 | 200 |  232.293667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:16 | 200 |  1.013420167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:18 | 200 |  371.294667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:19 | 200 |  224.743459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:19 | 200 |  234.556167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:27 | 200 |  351.511875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:30 | 200 |  770.988333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:30 | 200 |  219.517125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:31 | 200 |  219.803125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:39 | 200 |  347.036917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:41 | 200 |  652.932792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:42 | 200 |  188.792291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:42 | 200 |  213.854917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:50 | 200 |  346.706709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:53 | 200 |  396.416541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:53 | 200 |  227.390125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:52:53 | 200 |  212.903333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:02 | 200 |  588.284291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:04 | 200 |  266.529083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:04 | 200 |  336.646083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:04 | 200 |  460.150458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:13 | 200 |  547.620708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:15 | 200 |  214.464625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:16 | 200 |  153.400167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:16 | 200 |    339.3195ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:25 | 200 |  949.303416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:27 | 200 |  230.232042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:27 | 200 |  228.212417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:27 | 200 |  340.002125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:37 | 200 |  322.384083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:38 | 200 |     232.984ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:38 | 200 |  255.557709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:39 | 200 |  639.363584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:48 | 200 |  365.319791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:49 | 200 |  229.554584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:49 | 200 |  239.604916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:53:50 | 200 |  392.879375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:00 | 200 |  612.582959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:01 | 200 |  231.711292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:01 | 200 |  403.194584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:02 | 200 |  383.724708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:11 | 200 |  326.715584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:12 | 200 |    225.7485ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:12 | 200 |  174.599833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:13 | 200 |  371.790792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:23 | 200 |  274.051958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:23 | 200 |  240.927333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:23 | 200 |  1.143430708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:25 | 200 |     417.334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:34 | 200 |  233.675417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:35 | 200 |  187.282709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:35 | 200 |  588.487125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:36 | 200 |  856.145542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:46 | 200 |  239.339583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:46 | 200 |  205.176375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:46 | 200 |  346.261083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:48 | 200 |  646.269334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:57 | 200 |  225.945875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:57 | 200 |   240.73225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:54:58 | 200 |    324.5245ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:00 | 200 |  375.451791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:08 | 200 |  227.800583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:08 | 200 |  234.460125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:10 | 200 |  998.316334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:11 | 200 |  368.888708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:20 | 200 |  236.257917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:20 | 200 |  240.929084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:21 | 200 |  344.766833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:22 | 200 |  360.442792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:31 | 200 |  232.471458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:31 | 200 |  244.413833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:33 | 200 |  559.842125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:34 | 200 |   363.90175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:42 | 200 |  230.816208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:42 | 200 |  235.929625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:44 | 200 |    319.5795ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:45 | 200 |   366.78375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:53 | 200 |  226.625208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:53 | 200 |  235.589583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:56 | 200 |  312.576708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:55:57 | 200 |   360.39425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:05 | 200 |  231.141042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:05 | 200 |  239.287833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:07 | 200 |  319.660667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:08 | 200 |  375.815583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:16 | 200 |  229.418041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:16 | 200 |     238.596ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:18 | 200 |  339.538958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:19 | 200 |  375.297792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:27 | 200 |  230.886291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:27 | 200 |  226.239167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:30 | 200 |  346.482166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:31 | 200 |   357.48675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:38 | 200 |  230.925208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:39 | 200 |  267.442833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:42 | 200 |  902.049833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:42 | 200 |  369.822583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:50 | 200 |  230.361208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:50 | 200 |  218.383792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:53 | 200 |  340.349583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:56:54 | 200 |  355.921625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:01 | 200 |  250.362667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:01 | 200 |  250.396166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:04 | 200 |  335.939833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:05 | 200 |  368.961959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:12 | 200 |   232.39975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:12 | 200 |  238.666167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:16 | 200 |   350.02375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:16 | 200 |  362.923458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:24 | 200 |  231.088833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:24 | 200 |  233.622334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:27 | 200 |  329.768542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:28 | 200 |  363.412209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:35 | 200 |  227.368667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:35 | 200 |  232.144458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:39 | 200 |  835.045083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:39 | 200 |  336.933583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:46 | 200 |  228.962792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:46 | 200 |     239.668ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:50 | 200 |   381.52875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:51 | 200 |  683.628166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:57 | 200 |  236.483625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:57:57 | 200 |  241.497667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:02 | 200 |  344.902458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:02 | 200 |    368.9455ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:09 | 200 |  244.761375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:09 | 200 |  256.657334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:13 | 200 |  329.309167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:14 | 200 |  365.152542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:20 | 200 |  233.158209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:20 | 200 |  266.094042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:25 | 200 |  351.399917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:25 | 200 |   366.69225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:31 | 200 |  241.583125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:31 | 200 |  218.114833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:36 | 200 |  307.093708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:37 | 200 |  350.970209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:42 | 200 |     212.584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:43 | 200 |  214.986417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:47 | 200 |  523.391208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:48 | 200 |  331.309625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:54 | 200 |  228.137917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:54 | 200 |  233.960875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:59 | 200 |  606.498584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:58:59 | 200 |  357.822416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:05 | 200 |  244.185167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:05 | 200 |  260.794791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:11 | 200 |  369.416208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:11 | 200 |  364.896084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:16 | 200 |  222.109959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:16 | 200 |  228.493959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:22 | 200 |   401.37775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:22 | 200 |  385.479583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:28 | 200 |    221.1845ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:28 | 200 |  225.561083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:34 | 200 |  503.489125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:34 | 200 |  945.187708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:39 | 200 |  225.699334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:39 | 200 |  225.276042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:45 | 200 |     355.376ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:46 | 200 |  910.964834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:50 | 200 |  224.994375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:50 | 200 |  236.262459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:56 | 200 |  362.570834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 02:59:57 | 200 |  341.555459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:01 | 200 |  257.722042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:01 | 200 |  265.909875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:08 | 200 |  360.223167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:09 | 200 |   551.47125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:13 | 200 |  232.423333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:13 | 200 |  248.349792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:19 | 200 |  365.811458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:20 | 200 |   336.69875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:24 | 200 |  229.768042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:24 | 200 |  239.250625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:31 | 200 |  837.048875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:32 | 200 |  572.052291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:35 | 200 |  254.628375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:35 | 200 |  259.727666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:43 | 200 |  360.776917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:44 | 200 |  653.791167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:47 | 200 |  233.660791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:47 | 200 |  241.150792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:54 | 200 |  372.991958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:55 | 200 |  328.782792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:58 | 200 |  213.919959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:00:58 | 200 |  227.789667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:05 | 200 |    361.5545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:06 | 200 |   362.68975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:09 | 200 |    248.7645ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:09 | 200 |  247.968708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:17 | 200 |     373.658ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:18 | 200 |   414.07425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:20 | 200 |  250.666125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:20 | 200 |  254.642333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:28 | 200 |   354.77575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:29 | 200 |  367.437791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:32 | 200 |   234.96975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:32 | 200 |     243.948ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:40 | 200 |  1.021425167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:41 | 200 |  314.422292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:43 | 200 |  236.153291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:43 | 200 |  251.484416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:52 | 200 |  364.706958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:52 | 200 |  534.741792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:54 | 200 |  233.998458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:01:54 | 200 |  239.828958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:03 | 200 |  360.366875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:04 | 200 |  907.666083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:05 | 200 |  223.956541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:06 | 200 |  224.255958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:14 | 200 |  386.267542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:15 | 200 |  350.235125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:17 | 200 |  230.286584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:17 | 200 |  236.625375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:27 | 200 |    1.0656445s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:27 | 200 |  286.444625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:28 | 200 |  231.763458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:28 | 200 |  237.623583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:38 | 200 |  451.738458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:39 | 200 |  825.198958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:39 | 200 |  246.829625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:39 | 200 |  249.469791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:49 | 200 |  370.834375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:50 | 200 |  331.195958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:51 | 200 |  416.958042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:02:51 | 200 |  413.081917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:01 | 200 |  363.525625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:01 | 200 |  331.496834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:02 | 200 |  231.824167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:02 | 200 |  234.745791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:12 | 200 |  369.049792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:13 | 200 |  347.079834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:13 | 200 |  226.100583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:13 | 200 |  240.335167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:24 | 200 |  364.183917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:24 | 200 |  332.647875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:25 | 200 |  229.325459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:25 | 200 |  237.244792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:35 | 200 |  365.419917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:36 | 200 |  335.830792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:36 | 200 |  170.289583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:36 | 200 |  168.900583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:47 | 200 |  371.649291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:47 | 200 |  343.422375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:47 | 200 |  233.647375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:47 | 200 |  266.350458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:58 | 200 |  387.617375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:59 | 200 |  574.220208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:59 | 200 |  498.498541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:03:59 | 200 |  1.308316292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:10 | 200 |   214.09975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:10 | 200 |   235.84825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:10 | 200 |   673.55325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:10 | 200 |    445.2785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:21 | 200 |  223.380333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:21 | 200 |  162.566583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:22 | 200 |  417.342584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:22 | 200 |  422.483875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:32 | 200 |  219.726291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:32 | 200 |   160.78975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:33 | 200 |  440.589125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:33 | 200 |  447.061084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:43 | 200 |  237.073875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:44 | 200 |  201.811041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:45 | 200 |  642.995042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:45 | 200 |  1.040049125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:55 | 200 |   230.03625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:55 | 200 |  249.131625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:56 | 200 |  369.421125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:04:57 | 200 |  952.451958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:06 | 200 |  229.259542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:06 | 200 |  239.770209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:08 | 200 |  364.470875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:09 | 200 |  595.198125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:17 | 200 |  230.632542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:17 | 200 |  243.452708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:19 | 200 |  375.886125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:20 | 200 |  333.415709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:29 | 200 |     228.536ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:29 | 200 |  241.589625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:31 | 200 |  365.386084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:32 | 200 |  984.521833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:40 | 200 |  229.714875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:40 | 200 |  239.411125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:42 | 200 |     373.536ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:44 | 200 |  327.850875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:51 | 200 |  223.067791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:51 | 200 |  234.158667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:53 | 200 |  372.594209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:05:55 | 200 |  322.633292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:02 | 200 |  222.946584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:03 | 200 |  233.311667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:05 | 200 |  359.342792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:06 | 200 |  317.603042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:14 | 200 |   226.61375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:14 | 200 |  227.575667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:16 | 200 |   373.91225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:18 | 200 |  543.656459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:25 | 200 |   222.97875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:25 | 200 |  237.618167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:28 | 200 |  375.350166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:30 | 200 |  1.017486375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:36 | 200 |  210.002417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:36 | 200 |   206.29675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:39 | 200 |  370.435125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:41 | 200 |  333.814208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:47 | 200 |  230.338125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:48 | 200 |  239.137833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:51 | 200 |  367.301125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:53 | 200 |  340.221708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:59 | 200 |  224.065167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:06:59 | 200 |  230.517625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:02 | 200 |  365.667084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:04 | 200 |  316.954709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:10 | 200 |  223.288417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:10 | 200 |  253.432209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:14 | 200 |    843.5265ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:16 | 200 |  334.116625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:21 | 200 |  226.591709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:21 | 200 |    219.4825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:25 | 200 |   363.44425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:27 | 200 |  324.362416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:33 | 200 |   226.92325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:33 | 200 |  235.897541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:37 | 200 |  366.883916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:38 | 200 |  308.915166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:44 | 200 |  230.862208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:44 | 200 |  247.066042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:48 | 200 |  360.382708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:50 | 200 |  315.893875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:55 | 200 |  223.807625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:55 | 200 |  229.749833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:07:59 | 200 |  363.880541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:01 | 200 |  317.896291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:06 | 200 |  225.774417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:06 | 200 |   232.75075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:11 | 200 |    366.8655ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:13 | 200 |   535.41325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:18 | 200 |  226.167959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:18 | 200 |  238.752541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:22 | 200 |    370.2315ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:24 | 200 |  340.756875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:29 | 200 |  224.217958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:29 | 200 |  228.567375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:34 | 200 |  368.587708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:35 | 200 |  334.039375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:40 | 200 |   246.43625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:40 | 200 |  254.272875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:45 | 200 |  371.738125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:47 | 200 |  363.806333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:51 | 200 |  224.861625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:52 | 200 |  228.160209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:57 | 200 |  365.335708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:08:58 | 200 |  340.695334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:03 | 200 |  227.147167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:03 | 200 |   236.49425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:08 | 200 |  363.073459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:10 | 200 |  841.507958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:14 | 200 |  229.548042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:14 | 200 |     240.933ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:19 | 200 |    381.0785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:22 | 200 |  372.791166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:25 | 200 |  242.829292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:25 | 200 |   235.26175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:31 | 200 |  359.029459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:33 | 200 |  334.740417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:37 | 200 |  214.833334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:37 | 200 |     225.502ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:42 | 200 |     359.882ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:44 | 200 |  313.961459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:48 | 200 |  234.268083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:48 | 200 |  238.514709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:54 | 200 |  373.772708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:56 | 200 |  848.362083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:59 | 200 |  231.876041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:09:59 | 200 |  232.477917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:05 | 200 |  381.570625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:08 | 200 |  344.584959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:10 | 200 |  239.674125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:10 | 200 |  250.623292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:16 | 200 |  353.004958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:19 | 200 |  338.477625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:22 | 200 |   228.01775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:22 | 200 |   248.10875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:28 | 200 |  365.123167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:30 | 200 |  334.665083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:33 | 200 |  218.279875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:33 | 200 |  201.513542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:39 | 200 |  366.386042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:43 | 200 |  1.385314375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:44 | 200 |  223.215875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:44 | 200 |  218.790333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:51 | 200 |  368.503166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:54 | 200 |     320.028ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:55 | 200 |  208.843416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:10:56 | 200 |   228.02325ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:02 | 200 |  371.660916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:05 | 200 |  318.006375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:07 | 200 |  230.417208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:07 | 200 |    222.7465ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:13 | 200 |  367.609708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:17 | 200 |  783.919875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:18 | 200 |  215.258667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:18 | 200 |  212.265125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:25 | 200 |  374.831708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:29 | 200 |  319.377792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:29 | 200 |  210.551708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:29 | 200 |  207.953917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:36 | 200 |  370.726209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:40 | 200 |  577.905667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:40 | 200 |  192.149917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:40 | 200 |  194.939625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:48 | 200 |  791.788125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:52 | 200 |  278.785791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:52 | 200 |  413.131875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:52 | 200 |     908.078ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:11:59 | 200 |  379.877458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:03 | 200 |  224.422333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:03 | 200 |  149.847166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:03 | 200 |  302.015583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:11 | 200 |  381.918834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:14 | 200 |  387.797292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:14 | 200 |    329.5085ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:15 | 200 |  919.327333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:22 | 200 |     383.563ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:26 | 200 |  231.452083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:26 | 200 |  240.715667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:27 | 200 |  854.013375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:34 | 200 |   381.45925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:37 | 200 |  229.569625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:37 | 200 |     237.702ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:39 | 200 |  367.591709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:44 | 200 |      401.75µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 03:12:44 | 200 |    6.773084ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 03:12:45 | 200 |   357.82475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:48 | 200 |  228.986875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:48 | 200 |  232.681708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:50 | 200 |  345.867166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:57 | 200 |  368.111666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:12:59 | 200 |  215.466792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:00 | 200 |  217.635291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:02 | 200 |  712.407625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:05 | 200 |      38.166µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 03:13:06 | 200 |   47.241959ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/12/31 - 03:13:06 | 200 |  738.868958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:08 | 200 |   528.80675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:11 | 200 |    234.6475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:11 | 200 |  239.769584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:13 | 200 |  329.651958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:19 | 200 |  350.677791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:22 | 200 |  212.111916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:22 | 200 |  212.572792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:25 | 200 |     351.652ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:31 | 200 |  370.136542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:33 | 200 |  237.094708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:33 | 200 |  259.233083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:36 | 200 |  351.181166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:42 | 200 |  366.804416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:45 | 200 |  233.703959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:45 | 200 |  237.887917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:48 | 200 |  612.538541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:54 | 200 |  369.071542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:56 | 200 |  213.986417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:56 | 200 |   227.48425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:13:59 | 200 |  320.151375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:05 | 200 |  366.339458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:07 | 200 |  228.325375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:07 | 200 |   215.65925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:10 | 200 |  341.119667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:17 | 200 |  698.075125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:18 | 200 |  227.385708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:18 | 200 |     235.263ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:22 | 200 |  347.409583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:28 | 200 |   362.78225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:30 | 200 |  222.600375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:30 | 200 |  227.319208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:33 | 200 |  347.883291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:40 | 200 |  364.475333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:41 | 200 |  228.103167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:41 | 200 |  237.450375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:45 | 200 |  1.143514125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:51 | 200 |  346.095709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:52 | 200 |  237.074042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:52 | 200 |  260.654083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:14:57 | 200 |   331.28375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:02 | 200 |  349.951666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:03 | 200 |  211.901333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:03 | 200 |  186.730083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:08 | 200 |  328.882208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:14 | 200 |  383.213792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:15 | 200 |    211.6705ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:15 | 200 |  218.112458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:19 | 200 |  356.922875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:25 | 200 |  364.274125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:26 | 200 |    234.1055ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:26 | 200 |  245.407041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:31 | 200 |  345.542083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:37 | 200 |  363.220458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:37 | 200 |  231.483334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:37 | 200 |   236.57825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:42 | 200 |     624.515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:48 | 200 |  370.052084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:48 | 200 |  263.340916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:49 | 200 |  277.954125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:15:54 | 200 |  348.240667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:00 | 200 |  368.614042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:00 | 200 |  916.242042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:00 | 200 |  447.389333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:05 | 200 |  345.249459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:11 | 200 |  234.052667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:11 | 200 |  156.856666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:11 | 200 |  378.402916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:17 | 200 |  970.482958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:22 | 200 |  210.735417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:22 | 200 |  208.330458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:23 | 200 |  585.159917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:29 | 200 |  359.127584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:34 | 200 |   227.78375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:34 | 200 |  245.937333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:35 | 200 |  1.199237584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:40 | 200 |  341.957958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:45 | 200 |  230.777708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:45 | 200 |  271.357334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:47 | 200 |  393.076666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:51 | 200 |      332.47ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:56 | 200 |  233.743375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:56 | 200 |  216.499917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:16:58 | 200 |  372.099167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:03 | 200 |     557.134ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:08 | 200 |  240.339959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:08 | 200 |  251.887208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:10 | 200 |  385.497125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:15 | 200 |  923.789875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:19 | 200 |  234.550875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:19 | 200 |  243.965875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:21 | 200 |  383.223167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:27 | 200 |    771.5145ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:30 | 200 |    214.3855ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:30 | 200 |   218.20375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:32 | 200 |   385.09925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:38 | 200 |  341.660125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:41 | 200 |    233.0575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:41 | 200 |  230.550584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:44 | 200 |  382.870042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:50 | 200 |  812.626042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:53 | 200 |  238.988875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:53 | 200 |  249.702958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:17:56 | 200 |  622.625875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:02 | 200 |   912.95225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:04 | 200 |  247.353167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:04 | 200 |  265.517541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:07 | 200 |  370.302375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:13 | 200 |  331.534875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:15 | 200 |  213.291916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:15 | 200 |  210.184583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:18 | 200 |  385.376167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:25 | 200 |  321.494625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:26 | 200 |  238.342583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:27 | 200 |  247.140125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:30 | 200 |  793.217916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:36 | 200 |  309.197917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:38 | 200 |  237.603917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:38 | 200 |  246.794791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:42 | 200 |  375.123792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:47 | 200 |  334.867042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:49 | 200 |  227.505917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:49 | 200 |  228.580666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:53 | 200 |  369.399875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:18:59 | 200 |  349.480125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:00 | 200 |  235.070375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:00 | 200 |  270.598834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:04 | 200 |  383.882375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:10 | 200 |  320.690042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:12 | 200 |  230.048958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:12 | 200 |     204.428ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:16 | 200 |  387.592292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:22 | 200 |   342.94825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:23 | 200 |  229.751667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:23 | 200 |    238.1175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:28 | 200 |  651.490708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:33 | 200 |  345.459084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:34 | 200 |     215.533ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:34 | 200 |   223.79575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:39 | 200 |  376.961584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:45 | 200 |  771.045875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:45 | 200 |  226.748458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:45 | 200 |  225.450209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:50 | 200 |  389.498666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:57 | 200 |  333.556375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:57 | 200 |    898.7785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:19:57 | 200 |  389.991375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:02 | 200 |  368.883042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:08 | 200 |  255.466959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:08 | 200 |  296.637916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:08 | 200 |  369.404125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:13 | 200 |  390.401042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:19 | 200 |  213.269583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:19 | 200 |   155.04175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:20 | 200 |  790.294458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:25 | 200 |     367.061ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:30 | 200 |  232.623833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:31 | 200 |  205.033375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:31 | 200 |  316.040041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:36 | 200 |  728.034333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:42 | 200 |  228.921917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:42 | 200 |  241.136083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:43 | 200 |  555.591709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:48 | 200 |  386.925791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:53 | 200 |     223.307ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:53 | 200 |  230.899375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:55 | 200 |  570.896167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:20:59 | 200 |     381.472ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:04 | 200 |   227.70375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:04 | 200 |  244.273459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:07 | 200 |  886.335875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:11 | 200 |   803.80075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:16 | 200 |  231.252542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:16 | 200 |  262.045208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:18 | 200 |  326.142125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:23 | 200 |  364.720709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:27 | 200 |  231.194334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:27 | 200 |  219.377875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:30 | 200 |  974.226583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:34 | 200 |  365.785375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:38 | 200 |  228.195583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:38 | 200 |  237.274791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:41 | 200 |  544.603083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:45 | 200 |     370.634ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:49 | 200 |  232.140958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:50 | 200 |  237.374125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:53 | 200 |  350.650791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:21:57 | 200 |  365.704625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:01 | 200 |  231.360459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:01 | 200 |  249.454834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:04 | 200 |  327.284958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:08 | 200 |  366.923167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:12 | 200 |  232.096584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:12 | 200 |  247.368625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:16 | 200 |    355.5195ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:20 | 200 |  363.807167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:23 | 200 |  233.025042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:23 | 200 |  236.964166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:27 | 200 |  572.077917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:31 | 200 |  365.554792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:35 | 200 |    216.6135ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:35 | 200 |  226.243333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:39 | 200 |  339.617667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:42 | 200 |  358.158333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:46 | 200 |  237.045292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:46 | 200 |  270.043375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:50 | 200 |  543.840042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:54 | 200 |  360.304084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:57 | 200 |  226.375625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:22:57 | 200 |  220.958208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:02 | 200 |  336.077833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:05 | 200 |  365.189833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:08 | 200 |  234.940167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:08 | 200 |  239.734375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:14 | 200 |  794.321875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:17 | 200 |  365.146291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:20 | 200 |  233.932375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:20 | 200 |  235.635708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:25 | 200 |  906.587917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:28 | 200 |   371.05625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:31 | 200 |  232.997333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:31 | 200 |  248.169917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:37 | 200 |   595.80575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:40 | 200 |  361.876667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:42 | 200 |  230.322167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:42 | 200 |  238.004542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:49 | 200 |  945.679042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:51 | 200 |   766.32525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:54 | 200 |  239.307916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:23:54 | 200 |  258.619625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:01 | 200 |  758.407416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:03 | 200 |  369.535208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:05 | 200 |  240.803708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:05 | 200 |  247.146208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:12 | 200 |  322.186792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:15 | 200 |  856.616458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:16 | 200 |  232.620958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:16 | 200 |   243.20075ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:24 | 200 |  1.001159792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:26 | 200 |  376.600291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:27 | 200 |  233.431542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:27 | 200 |  258.344875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:36 | 200 |  874.805709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:38 | 200 |  369.682166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:39 | 200 |  228.296833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:39 | 200 |  228.200833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:48 | 200 |  345.104041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:49 | 200 |   375.33975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:50 | 200 |  219.106041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:50 | 200 |  200.188209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:24:59 | 200 |  363.166917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:00 | 200 |  392.782791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:01 | 200 |  237.358667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:01 | 200 |     254.041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:10 | 200 |  345.728917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:12 | 200 |   922.55825ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:12 | 200 |  196.583167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:13 | 200 |  216.342167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:22 | 200 |  323.942458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:24 | 200 |  265.671125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:24 | 200 |  494.010709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:24 | 200 |   307.56175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:33 | 200 |  323.912208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:35 | 200 |  226.902458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:35 | 200 |  173.612792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:35 | 200 |  350.035041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:45 | 200 |    321.7275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:46 | 200 |  214.902375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:46 | 200 |  189.767209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:47 | 200 |     330.987ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:57 | 200 |  938.282833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:57 | 200 |  209.093167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:58 | 200 |  209.016083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:25:59 | 200 |  947.233583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:08 | 200 |  326.759042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:09 | 200 |  212.494458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:09 | 200 |  223.697292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:10 | 200 |  367.938791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:19 | 200 |  337.273625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:20 | 200 |  207.867625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:20 | 200 |    208.3545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:21 | 200 |  366.312459ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:31 | 200 |  349.278625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:31 | 200 |  207.703417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:31 | 200 |  208.117667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:33 | 200 |  370.761875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:42 | 200 |  329.101667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:42 | 200 |  233.487541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:43 | 200 |  236.716166ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:44 | 200 |  390.984084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:53 | 200 |  317.595709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:54 | 200 |  232.986542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:54 | 200 |   265.41475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:26:56 | 200 |  712.550709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:05 | 200 |  354.137291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:05 | 200 |  191.409583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:05 | 200 |  169.422208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:07 | 200 |  388.738417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:16 | 200 |  343.563833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:16 | 200 |  208.414291ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:16 | 200 |  224.968292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:19 | 200 |   388.33975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:28 | 200 |  281.382458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:28 | 200 |   403.40375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:28 | 200 |  288.157167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:30 | 200 |  393.256333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:39 | 200 |  295.464542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:39 | 200 |  397.156667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:39 | 200 |  354.849667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:42 | 200 |  388.621375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:50 | 200 |  225.594292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:50 | 200 |  259.637167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:50 | 200 |  352.791792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:27:53 | 200 |  383.938625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:01 | 200 |   228.55925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:02 | 200 |  139.261625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:02 | 200 |  881.696375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:05 | 200 |  702.640041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:13 | 200 |  228.848667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:13 | 200 |  241.192333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:14 | 200 |  951.442125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:16 | 200 |  376.485333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:24 | 200 |  229.300791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:24 | 200 |   235.84125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:26 | 200 |  365.599083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:28 | 200 |  369.692208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:35 | 200 |     205.423ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:35 | 200 |  205.356792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:37 | 200 |  323.840542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:39 | 200 |  360.154458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:46 | 200 |  229.818416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:47 | 200 |  236.191375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:49 | 200 |   927.96875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:51 | 200 |  371.017833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:58 | 200 |  229.939541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:28:58 | 200 |  234.929834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:01 | 200 |  794.972417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:02 | 200 |   363.06275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:09 | 200 |  230.674834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:09 | 200 |    225.6015ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:12 | 200 |  326.806458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:13 | 200 |  377.398333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:20 | 200 |  221.721458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:20 | 200 |     219.849ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:24 | 200 |    976.8965ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:25 | 200 |  353.046208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:32 | 200 |   246.94525ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:32 | 200 |  259.990917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:36 | 200 |      581.77ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:37 | 200 |  1.154878333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:43 | 200 |  228.800042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:43 | 200 |  233.642792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:48 | 200 |  483.038458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:48 | 200 |  419.079791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:54 | 200 |  237.587208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:54 | 200 |  245.035584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:59 | 200 |  464.872167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:29:59 | 200 |  534.931041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:05 | 200 |  224.452167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:05 | 200 |  229.455209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:11 | 200 |  459.785375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:11 | 200 |  493.371125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:17 | 200 |   216.59625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:17 | 200 |  220.124167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:22 | 200 |   449.38775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:23 | 200 |  469.998958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:28 | 200 |  227.101042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:28 | 200 |  230.742792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:34 | 200 |   435.32475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:34 | 200 |  465.885292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:39 | 200 |     251.683ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:39 | 200 |  256.319416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:45 | 200 |   433.09925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:45 | 200 |  433.666041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:50 | 200 |  236.263417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:51 | 200 |  238.126875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:57 | 200 |  453.588084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:30:57 | 200 |  451.757625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:02 | 200 |  231.418625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:02 | 200 |   241.07175ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:09 | 200 |  539.124041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:09 | 200 |  1.089536417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:13 | 200 |  219.275792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:13 | 200 |      222.05ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:20 | 200 |  348.974541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:21 | 200 |  1.403840333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:24 | 200 |  220.066708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:24 | 200 |  216.453042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:32 | 200 |   591.85025ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:33 | 200 |    337.5875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:36 | 200 |  262.377959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:36 | 200 |   302.36775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:43 | 200 |  356.534125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:44 | 200 |   545.17475ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:47 | 200 |  233.860084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:47 | 200 |  219.012209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:55 | 200 |  1.023903125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:56 | 200 |  345.496709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:58 | 200 |  238.928208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:31:58 | 200 |  245.430958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:07 | 200 |  639.098625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:07 | 200 |  328.511375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:09 | 200 |  241.370125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:09 | 200 |     247.222ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:18 | 200 |  366.437666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:19 | 200 |  910.644417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:21 | 200 |  245.268792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:21 | 200 |  253.973584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:29 | 200 |  355.247375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:31 | 200 |  359.827375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:32 | 200 |  238.649625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:32 | 200 |  245.534416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:41 | 200 |  360.680041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:42 | 200 |  561.108959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:43 | 200 |  231.749708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:43 | 200 |    234.5975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:53 | 200 |    897.6575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:54 | 200 |  552.380791ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:54 | 200 |  230.574583ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:32:55 | 200 |   233.10225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:04 | 200 |  380.238125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:05 | 200 |  361.000458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:06 | 200 |  225.358333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:06 | 200 |  230.072916ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:16 | 200 |  372.418833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:17 | 200 |  571.209709ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:17 | 200 |  195.598292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:17 | 200 |   211.20575ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:27 | 200 |   374.17775ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:28 | 200 |  341.057375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:28 | 200 |  219.501875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:28 | 200 |  222.963542ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:38 | 200 |  361.594708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:40 | 200 |   381.39725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:40 | 200 |  270.134917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:40 | 200 |  267.485666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:50 | 200 |  365.325584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:51 | 200 |  300.801917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:51 | 200 |  423.876083ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:33:51 | 200 |  391.492208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:01 | 200 |  360.030167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:02 | 200 |  254.041334ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:02 | 200 |  273.932375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:02 | 200 |  377.381292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:13 | 200 |  370.510042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:13 | 200 |  228.215959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:14 | 200 |  151.840958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:14 | 200 |  298.562041ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:24 | 200 |  457.477125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:25 | 200 |  211.319292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:25 | 200 |  194.874667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:25 | 200 |  636.029417ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:36 | 200 |  356.074625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:36 | 200 |  230.907959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:36 | 200 |  236.806834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:37 | 200 |  304.006209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:47 | 200 |  346.741833ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:47 | 200 |    174.6565ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:47 | 200 |  178.368625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:48 | 200 |  517.540208ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:58 | 200 |  404.195875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:58 | 200 |  261.934167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:34:59 | 200 |  269.126292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:00 | 200 |  305.916333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:10 | 200 |  267.644834ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:10 | 200 |  462.115875ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:10 | 200 |  338.272333ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:11 | 200 |  311.428584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:21 | 200 |  201.941292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:21 | 200 |  170.707917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:21 | 200 |  340.743792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:23 | 200 |  571.047667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:32 | 200 |  437.100458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:33 | 200 |  415.327584ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:33 | 200 |  462.699042ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 03:35:34 | 200 |  516.748209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:05:35 | 200 |       208.5µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 05:05:35 | 200 |   16.786375ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 05:06:47 | 200 |      30.042µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 05:06:47 | 200 |   57.127292ms |       127.0.0.1 | POST     "/api/show"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T05:06:59.425-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 61495"
time=2025-12-31T05:06:59.430-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="14.4 GiB" free_swap="0 B"
time=2025-12-31T05:06:59.430-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T05:06:59.430-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T05:06:59.431-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-31T05:06:59.431-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-31T05:06:59.431-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-31T05:06:59.431-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-31T05:06:59.502-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T05:06:59.503-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T05:06:59.540-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:61495"
time=2025-12-31T05:06:59.543-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T05:06:59.543-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T05:06:59.544-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-31T05:07:01.805-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.38 seconds"
time=2025-12-31T05:07:01.805-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T05:07:01.805-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T05:07:01.805-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.38 seconds"
[GIN] 2025/12/31 - 05:07:06 | 200 |  6.781905875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:09:13 | 200 |  8.393055584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:09:27 | 200 | 13.826254042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:09:47 | 200 | 20.416862708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:10:07 | 200 | 19.687351917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:10:24 | 200 | 17.441176709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 05:28:12 | 200 |      22.584µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 05:28:12 | 200 |    8.084708ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 11:34:07 | 200 |      28.417µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/12/31 - 11:34:07 | 200 |   10.229584ms |       127.0.0.1 | GET      "/api/tags"
[GIN] 2025/12/31 - 11:34:18 | 200 |    3.135667ms |       127.0.0.1 | GET      "/api/tags"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T11:34:21.131-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 54597"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="13.7 GiB" free_swap="0 B"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T11:34:21.136-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-31T11:34:21.136-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-31T11:34:21.146-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.017 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T11:34:21.147-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T11:34:21.223-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:54597"
time=2025-12-31T11:34:21.234-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T11:34:21.234-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:21.235-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-31T11:34:23.498-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.36 seconds"
time=2025-12-31T11:34:23.498-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T11:34:23.498-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:23.498-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.36 seconds"
[GIN] 2025/12/31 - 11:34:25 | 200 |  4.245306083s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:34:43.209-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="13.2 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T11:34:43.374-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 54668"
time=2025-12-31T11:34:43.380-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="6.6 GiB" free_swap="0 B"
time=2025-12-31T11:34:43.380-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="12.7 GiB" free="13.2 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T11:34:43.380-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T11:34:43.381-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="1.9 GiB"
time=2025-12-31T11:34:43.381-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="448.0 MiB"
time=2025-12-31T11:34:43.381-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="256.5 MiB"
time=2025-12-31T11:34:43.381-05:00 level=INFO source=device.go:272 msg="total memory" size="2.6 GiB"
time=2025-12-31T11:34:43.401-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.020 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T11:34:43.401-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T11:34:43.440-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:54668"
time=2025-12-31T11:34:43.447-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T11:34:43.447-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:43.447-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
load_tensors: Metal_Mapped model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      Metal compute buffer size =   262.50 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1014
llama_context: graph splits = 2
time=2025-12-31T11:34:44.702-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.32 seconds"
time=2025-12-31T11:34:44.702-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=2
time=2025-12-31T11:34:44.702-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:44.702-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.32 seconds"
[GIN] 2025/12/31 - 11:34:44 | 200 |  1.816720042s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:34:45.633-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="10.6 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T11:34:45.779-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 --port 54678"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="5.1 GiB" free_swap="0 B"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="10.1 GiB" free="10.6 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=33 requested=-1
time=2025-12-31T11:34:45.786-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.3 GiB"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="1.0 GiB"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="560.0 MiB"
time=2025-12-31T11:34:45.786-05:00 level=INFO source=device.go:272 msg="total memory" size="5.8 GiB"
time=2025-12-31T11:34:45.801-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.021 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T11:34:45.802-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T11:34:45.860-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:54678"
time=2025-12-31T11:34:45.863-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:8192 KvCacheType: NumThreads:10 GPULayers:33[ID:0 Layers:33(0..32)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T11:34:45.863-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:45.864-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 8B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 32
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.58 GiB (4.89 BPW) 
[GIN] 2025/12/31 - 11:34:45 | 200 |  5.040750792s |       127.0.0.1 | POST     "/api/generate"
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 4096
print_info: n_layer          = 32
print_info: n_head           = 32
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 4
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 14336
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 8B
print_info: model params     = 8.03 B
print_info: general.name     = Meta Llama 3.1 8B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 32 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 33/33 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
load_tensors: Metal_Mapped model buffer size =  4403.49 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 8192
llama_context: n_ctx_per_seq = 8192
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =  1024.00 MiB
llama_kv_cache: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_context:      Metal compute buffer size =   548.01 MiB
llama_context:        CPU compute buffer size =    28.01 MiB
llama_context: graph nodes  = 1158
llama_context: graph splits = 2
time=2025-12-31T11:34:48.628-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.84 seconds"
time=2025-12-31T11:34:48.628-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=3
time=2025-12-31T11:34:48.628-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:34:48.629-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.84 seconds"
[GIN] 2025/12/31 - 11:35:04 | 200 | 19.331159417s |       127.0.0.1 | POST     "/api/generate"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T11:45:04.061-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 --port 55659"
time=2025-12-31T11:45:04.066-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="13.5 GiB" free_swap="0 B"
time=2025-12-31T11:45:04.066-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="17.3 GiB" free="17.8 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T11:45:04.066-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T11:45:04.066-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="4.1 GiB"
time=2025-12-31T11:45:04.067-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="224.0 MiB"
time=2025-12-31T11:45:04.067-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="304.0 MiB"
time=2025-12-31T11:45:04.067-05:00 level=INFO source=device.go:272 msg="total memory" size="4.6 GiB"
time=2025-12-31T11:45:04.123-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.016 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T11:45:04.124-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T11:45:04.156-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:55659"
time=2025-12-31T11:45:04.167-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T11:45:04.168-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:45:04.168-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 34 key-value pairs and 339 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-60e05f2100071479f596b964f89f510f057ce397ea22f2833a0cfe029bfc2463 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 Coder 7B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5-Coder
llama_model_loader: - kv   5:                         general.size_label str              = 7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv   8:                   general.base_model.count u32              = 1
llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 Coder 7B
llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen
llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-C...
llama_model_loader: - kv  12:                               general.tags arr[str,6]       = ["code", "codeqwen", "chat", "qwen", ...
llama_model_loader: - kv  13:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28
llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768
llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584
llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944
llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28
llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4
llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  22:                          general.file_type u32              = 15
llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645
llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - kv  33:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  141 tensors
llama_model_loader: - type q4_K:  169 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 4.36 GiB (4.91 BPW) 
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 22
load: token to piece cache size = 0.9310 MB
print_info: arch             = qwen2
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 3584
print_info: n_layer          = 28
print_info: n_head           = 28
print_info: n_head_kv        = 4
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 7
print_info: n_embd_k_gqa     = 512
print_info: n_embd_v_gqa     = 512
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 18944
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 7B
print_info: model params     = 7.62 B
print_info: general.name     = Qwen2.5 Coder 7B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 152064
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151645 '<|im_end|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   292.36 MiB
load_tensors: Metal_Mapped model buffer size =  4168.09 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.59 MiB
llama_kv_cache:      Metal KV buffer size =   224.00 MiB
llama_kv_cache: size =  224.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  112.00 MiB, V (f16):  112.00 MiB
llama_context:      Metal compute buffer size =   311.00 MiB
llama_context:        CPU compute buffer size =    17.01 MiB
llama_context: graph nodes  = 1098
llama_context: graph splits = 2
time=2025-12-31T11:45:06.430-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.36 seconds"
time=2025-12-31T11:45:06.430-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=1
time=2025-12-31T11:45:06.430-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:45:06.430-05:00 level=INFO source=server.go:1332 msg="llama runner started in 2.36 seconds"
[GIN] 2025/12/31 - 11:45:13 | 200 |  9.242676458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:45:23 | 200 | 19.676780667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:45:29 | 200 | 25.196712958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:45:37 | 200 | 33.654260959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:45:49 | 200 | 45.456657292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:45:54 | 200 | 25.797046916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:46:10 | 200 | 21.473762667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:46:18 | 200 |    23.884432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:46:35 | 200 | 24.723348541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:46:48 | 200 | 29.865154792s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:46:49.583-05:00 level=INFO source=server.go:1497 msg="aborting completion request due to client closing the connection"
[GIN] 2025/12/31 - 11:46:49 | 500 |  960.881416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:47:02 | 200 | 26.712203875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:47:10 | 200 | 20.160249917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:47:26 | 200 | 23.983999667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:47:48 | 200 | 46.209172666s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:48:10.040-05:00 level=INFO source=server.go:1497 msg="aborting completion request due to client closing the connection"
[GIN] 2025/12/31 - 11:48:10 | 500 |          1m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:48:12 | 200 |         1m10s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:48:38 | 200 |         1m36s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:48:50 | 200 | 40.310130834s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:49:13 | 200 |  35.42558225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:49:26 | 200 | 36.500989959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:49:49 | 200 | 35.066358083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:50:13 | 200 | 59.199318167s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:50:26.863-05:00 level=INFO source=server.go:1497 msg="aborting completion request due to client closing the connection"
[GIN] 2025/12/31 - 11:50:26 | 500 |          1m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:50:33 | 200 |         1m19s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:50:48 | 200 |         1m34s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:00 | 200 |  33.21121125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:04 | 200 |   4.83223125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:07 | 200 |  3.069259833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:11 | 200 |    3.8497265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:14 | 200 |  2.730149541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:24 | 200 | 10.191979458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:28 | 200 |  4.171453083s |       127.0.0.1 | POST     "/api/generate"
time=2025-12-31T11:51:29.013-05:00 level=INFO source=sched.go:583 msg="updated VRAM based on existing loaded models" gpu=0 library=Metal total="17.8 GiB" available="13.2 GiB"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
llama_model_load: vocab only - skipping tensors
time=2025-12-31T11:51:29.165-05:00 level=INFO source=server.go:392 msg="starting runner" cmd="/opt/homebrew/Cellar/ollama/0.13.1/bin/ollama runner --model /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --port 56014"
time=2025-12-31T11:51:29.170-05:00 level=INFO source=sched.go:443 msg="system memory" total="24.0 GiB" free="8.0 GiB" free_swap="0 B"
time=2025-12-31T11:51:29.170-05:00 level=INFO source=sched.go:450 msg="gpu memory" id=0 library=Metal available="12.7 GiB" free="13.2 GiB" minimum="512.0 MiB" overhead="0 B"
time=2025-12-31T11:51:29.170-05:00 level=INFO source=server.go:459 msg="loading model" "model layers"=29 requested=-1
time=2025-12-31T11:51:29.171-05:00 level=INFO source=device.go:240 msg="model weights" device=Metal size="1.9 GiB"
time=2025-12-31T11:51:29.171-05:00 level=INFO source=device.go:251 msg="kv cache" device=Metal size="448.0 MiB"
time=2025-12-31T11:51:29.171-05:00 level=INFO source=device.go:262 msg="compute graph" device=Metal size="256.5 MiB"
time=2025-12-31T11:51:29.171-05:00 level=INFO source=device.go:272 msg="total memory" size="2.6 GiB"
time=2025-12-31T11:51:29.222-05:00 level=INFO source=runner.go:963 msg="starting go runner"
ggml_metal_library_init: using embedded metal library
ggml_metal_library_init: loaded in 0.016 sec
ggml_metal_device_init: GPU name:   Apple M4 Pro
ggml_metal_device_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_device_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_device_init: GPU family: MTLGPUFamilyMetal4  (5002)
ggml_metal_device_init: simdgroup reduction   = true
ggml_metal_device_init: simdgroup matrix mul. = true
ggml_metal_device_init: has unified memory    = true
ggml_metal_device_init: has bfloat            = true
ggml_metal_device_init: use residency sets    = true
ggml_metal_device_init: use shared buffers    = true
ggml_metal_device_init: recommendedMaxWorkingSetSize  = 19069.67 MB
time=2025-12-31T11:51:29.222-05:00 level=INFO source=ggml.go:104 msg=system Metal.0.EMBED_LIBRARY=1 CPU.0.NEON=1 CPU.0.ARM_FMA=1 CPU.0.FP16_VA=1 CPU.0.DOTPROD=1 CPU.0.LLAMAFILE=1 CPU.0.ACCELERATE=1 compiler=cgo(clang)
time=2025-12-31T11:51:29.261-05:00 level=INFO source=runner.go:999 msg="Server listening on 127.0.0.1:56014"
time=2025-12-31T11:51:29.270-05:00 level=INFO source=runner.go:893 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:10 GPULayers:29[ID:0 Layers:29(0..28)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) (unknown id) - 18185 MiB free
time=2025-12-31T11:51:29.270-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:51:29.271-05:00 level=INFO source=server.go:1328 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/alexandercpaul/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_K - Medium
print_info: file size   = 1.87 GiB (5.01 BPW) 
load: printing all EOG tokens:
load:   - 128001 ('<|end_of_text|>')
load:   - 128008 ('<|eom_id|>')
load:   - 128009 ('<|eot_id|>')
load: special tokens cache size = 256
load: token to piece cache size = 0.7999 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 131072
print_info: n_embd           = 3072
print_info: n_layer          = 28
print_info: n_head           = 24
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 500000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 131072
print_info: rope_finetuned   = unknown
print_info: model type       = 3B
print_info: model params     = 3.21 B
print_info: general.name     = Llama 3.2 3B Instruct
print_info: vocab type       = BPE
print_info: n_vocab          = 128256
print_info: n_merges         = 280147
print_info: BOS token        = 128000 '<|begin_of_text|>'
print_info: EOS token        = 128009 '<|eot_id|>'
print_info: EOT token        = 128001 '<|end_of_text|>'
print_info: EOM token        = 128008 '<|eom_id|>'
print_info: LF token         = 198 'Ċ'
print_info: EOG token        = 128001 '<|end_of_text|>'
print_info: EOG token        = 128008 '<|eom_id|>'
print_info: EOG token        = 128009 '<|eot_id|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   308.23 MiB
load_tensors: Metal_Mapped model buffer size =  1918.35 MiB
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 500000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
ggml_metal_init: allocating
ggml_metal_init: picking default device: Apple M4 Pro
ggml_metal_init: use bfloat         = true
ggml_metal_init: use fusion         = true
ggml_metal_init: use concurrency    = true
ggml_metal_init: use graph optimize = true
llama_context:        CPU  output buffer size =     0.50 MiB
llama_kv_cache:      Metal KV buffer size =   448.00 MiB
llama_kv_cache: size =  448.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):  224.00 MiB, V (f16):  224.00 MiB
llama_context:      Metal compute buffer size =   262.50 MiB
llama_context:        CPU compute buffer size =    18.01 MiB
llama_context: graph nodes  = 1014
llama_context: graph splits = 2
time=2025-12-31T11:51:30.528-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.36 seconds"
time=2025-12-31T11:51:30.528-05:00 level=INFO source=sched.go:517 msg="loaded runners" count=2
time=2025-12-31T11:51:30.528-05:00 level=INFO source=server.go:1294 msg="waiting for llama runner to start responding"
time=2025-12-31T11:51:30.528-05:00 level=INFO source=server.go:1332 msg="llama runner started in 1.36 seconds"
[GIN] 2025/12/31 - 11:51:32 | 200 |  3.604996584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:37 | 200 |  4.845697041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:41 | 200 |  3.645047584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:47 | 200 |   6.05613625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/12/31 - 11:51:49 | 500 |  2.610128334s |       127.0.0.1 | POST     "/api/generate"
